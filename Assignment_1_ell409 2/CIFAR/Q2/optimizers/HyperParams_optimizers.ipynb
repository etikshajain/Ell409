{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5022,
     "status": "ok",
     "timestamp": 1695240476843,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "_btPg74EYNTh"
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri-Fm4O4VnDc"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695240476844,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "QcHzUTVnegru"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1 = nn.Linear(3*32*32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img): #convert + flatten\n",
    "        x = img.view(-1, 3*32*32)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQXUMoe0VpVz"
   },
   "source": [
    "\n",
    "\n",
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695240476844,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "SkaJoTYgamtY"
   },
   "outputs": [],
   "source": [
    "num_epochs=80\n",
    "after_every=100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7298,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "GJXtKqjIYUxl",
    "outputId": "ce115be3-93db-4097-a439-4efe96f48031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "\n",
    "mnist_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = t.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = t.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONzNfpu3VrPs"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "xMN-3GHIX-AE"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, epoch_index, optimizer, train_dataset):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        # Every data instance is an input + label pair\n",
    "        # data.shape=torch.Size([10, 1, 28, 28]) --> 10 images,labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs.view(-1, 3*32*32))\n",
    "\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % after_every == after_every-1:\n",
    "            last_loss = running_loss / after_every # loss per after_every batches\n",
    "            print(' batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkhUdxuTVtQ2"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "PaZP0GgJo_7Y"
   },
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "  total=0\n",
    "  correct=0\n",
    "  model.eval()\n",
    "  with t.no_grad():\n",
    "      for data in loader:\n",
    "          x, y = data\n",
    "          output = model(x.view(-1, 3*32*32))\n",
    "          for idx, i in enumerate(output):\n",
    "              if t.argmax(i) == y[idx]:\n",
    "                  correct +=1\n",
    "              total +=1\n",
    "  return round(correct/total, 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "qAaUlOcloXzr"
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dataset, test_dataset, optimizer, scheduler, LR_type):\n",
    "  epoch_number = 0\n",
    "\n",
    "  losses = []\n",
    "  train_accuracy = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "      # Make sure gradient tracking is on, and do a pass over the data\n",
    "      model.train(True)\n",
    "      avg_loss = train_one_epoch(model, epoch_number,optimizer,train_dataset)\n",
    "      losses.append(avg_loss)\n",
    "\n",
    "      # Train accuracy\n",
    "      train_acc = test(model, train_dataset)\n",
    "      train_accuracy.append(train_acc)\n",
    "\n",
    "      # Test accuracy\n",
    "      test_acc = test(model, test_dataset)\n",
    "      test_accuracy.append(test_acc)\n",
    "\n",
    "      if LR_type=='Plateau':\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step(avg_loss)\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print('lr {} -> {}'.format(before_lr, after_lr))\n",
    "\n",
    "      elif LR_type!='None':\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print('lr {} -> {}'.format(before_lr, after_lr))\n",
    "\n",
    "      print('Train accuracy {}:'.format(train_acc))\n",
    "      print('Test accuracy {}:'.format(test_acc))\n",
    "      epoch_number += 1\n",
    "  return losses, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25800,
     "status": "ok",
     "timestamp": 1695240509932,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "J-gvG1N1o46t",
    "outputId": "4e015ccc-6be8-4b74-a3d8-f5e929d4fffb"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "name = ['SGD', 'Adam', 'RMSProp']\n",
    "def train_with_optimizers(starting_lr, l2reg):\n",
    "  losses_arr = []\n",
    "  train_accuracy_arr = []\n",
    "  test_accuracy_arr = []\n",
    "  description = []\n",
    "  # Different Optimizers\n",
    "  for i,opti in enumerate(name):\n",
    "    print('Optimizer {}:'.format(opti))\n",
    "    net_new = Net()\n",
    "    optimizerr = opti\n",
    "    if(opti=='SGD'):\n",
    "        optimizerr = t.optim.SGD(net_new.parameters(), lr=starting_lr, weight_decay=l2reg)\n",
    "    if(opti=='Adam'):\n",
    "        optimizerr = t.optim.Adam(net_new.parameters(), lr=starting_lr, weight_decay=l2reg)\n",
    "    if(opti=='RMSProp'):\n",
    "        optimizerr = t.optim.RMSprop(net_new.parameters(), lr=starting_lr, weight_decay=l2reg)\n",
    "\n",
    "    scheduler = StepLR(optimizerr, 25, gamma=0.1)\n",
    "\n",
    "    losses, train_accuracy, test_accuracy = train_model(net_new, num_epochs, train_loader, test_loader, optimizerr, scheduler, 'Step')\n",
    "\n",
    "    losses_arr.append(losses)\n",
    "    train_accuracy_arr.append(train_accuracy)\n",
    "    test_accuracy_arr.append(test_accuracy)\n",
    "    description.append('Optimizer: {}, Starting LR:{}, L2Reg:{}, Batch size: {}'.format(name[i], starting_lr, l2reg, batch_size))\n",
    "\n",
    "    # save model\n",
    "    ts_now = dt.datetime.now()\n",
    "    unix_timestamp = dt.datetime.timestamp(ts_now)*1000\n",
    "    model_save_name = str(unix_timestamp)\n",
    "    path = F\"{model_save_name}_{name[i]}_L2Reg={l2reg}_bs{batch_size}_e{num_epochs}.pt\"\n",
    "    t.save(net.state_dict(), path)\n",
    "  return description, losses_arr, train_accuracy_arr, test_accuracy_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695240509932,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "8OGRXGLja8tY"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def save_to_csv(filename, description, losses_arr, train_acc_arr, test_acc_arr):\n",
    "  # Create a CSV file and write data to it\n",
    "  with open(filename+'.csv', 'w', newline='') as csvfile:\n",
    "    for i, desc in enumerate(description):\n",
    "      fieldnames = ['Description', 'epoch', 'train_loss', 'train_accuracy', 'test_accuracy']\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "      writer.writeheader()\n",
    "      for epoch, (loss, train, test) in enumerate(zip(losses_arr[i], train_acc_arr[i], test_acc_arr[i]), 1):\n",
    "          writer.writerow({'Description':description[i], 'epoch': epoch, 'train_loss': loss, 'train_accuracy': train, 'test_accuracy': test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l8SfkkYWymW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer SGD:\n",
      "EPOCH 1:\n",
      " batch 100 loss: 2.0413985228538514\n",
      " batch 200 loss: 1.8877138638496398\n",
      " batch 300 loss: 1.8379808449745179\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 39.1:\n",
      "Test accuracy 38.6:\n",
      "EPOCH 2:\n",
      " batch 100 loss: 1.7998032259941101\n",
      " batch 200 loss: 1.7789064347743988\n",
      " batch 300 loss: 1.7741796314716338\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 40.699999999999996:\n",
      "Test accuracy 39.7:\n",
      "EPOCH 3:\n",
      " batch 100 loss: 1.745538147687912\n",
      " batch 200 loss: 1.759667799472809\n",
      " batch 300 loss: 1.7545337545871735\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 41.8:\n",
      "Test accuracy 41.099999999999994:\n",
      "EPOCH 4:\n",
      " batch 100 loss: 1.751718716621399\n",
      " batch 200 loss: 1.7400790333747864\n",
      " batch 300 loss: 1.7172675836086273\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 42.0:\n",
      "Test accuracy 40.400000000000006:\n",
      "EPOCH 5:\n",
      " batch 100 loss: 1.7100052416324616\n",
      " batch 200 loss: 1.72964275598526\n",
      " batch 300 loss: 1.7411622047424316\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 42.3:\n",
      "Test accuracy 41.4:\n",
      "EPOCH 6:\n",
      " batch 100 loss: 1.7032789695262909\n",
      " batch 200 loss: 1.730118954181671\n",
      " batch 300 loss: 1.6996994996070862\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 42.8:\n",
      "Test accuracy 41.199999999999996:\n",
      "EPOCH 7:\n",
      " batch 100 loss: 1.699788589477539\n",
      " batch 200 loss: 1.721511173248291\n",
      " batch 300 loss: 1.6931298387050628\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 43.0:\n",
      "Test accuracy 41.0:\n",
      "EPOCH 8:\n",
      " batch 100 loss: 1.69297607421875\n",
      " batch 200 loss: 1.7024317049980164\n",
      " batch 300 loss: 1.6991588008403777\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 43.3:\n",
      "Test accuracy 41.3:\n",
      "EPOCH 9:\n",
      " batch 100 loss: 1.6866060638427733\n",
      " batch 200 loss: 1.6960312736034393\n",
      " batch 300 loss: 1.6995448505878448\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 43.4:\n",
      "Test accuracy 41.5:\n",
      "EPOCH 10:\n",
      " batch 100 loss: 1.6768394243717193\n",
      " batch 200 loss: 1.6910937273502349\n",
      " batch 300 loss: 1.689627743959427\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 43.6:\n",
      "Test accuracy 41.3:\n",
      "EPOCH 11:\n",
      " batch 100 loss: 1.6809870672225953\n",
      " batch 200 loss: 1.7018418753147124\n",
      " batch 300 loss: 1.6661241734027863\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 43.9:\n",
      "Test accuracy 41.699999999999996:\n",
      "EPOCH 12:\n",
      " batch 100 loss: 1.6797171878814696\n",
      " batch 200 loss: 1.6792603385448457\n",
      " batch 300 loss: 1.6911722218990326\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.2:\n",
      "Test accuracy 41.699999999999996:\n",
      "EPOCH 13:\n",
      " batch 100 loss: 1.668115714788437\n",
      " batch 200 loss: 1.6870724546909333\n",
      " batch 300 loss: 1.6672181808948516\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.2:\n",
      "Test accuracy 41.699999999999996:\n",
      "EPOCH 14:\n",
      " batch 100 loss: 1.6694013202190399\n",
      " batch 200 loss: 1.6796081972122192\n",
      " batch 300 loss: 1.6651633870601654\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 43.8:\n",
      "Test accuracy 41.099999999999994:\n",
      "EPOCH 15:\n",
      " batch 100 loss: 1.6592584764957428\n",
      " batch 200 loss: 1.6649168753623962\n",
      " batch 300 loss: 1.6809142208099366\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.5:\n",
      "Test accuracy 41.4:\n",
      "EPOCH 16:\n",
      " batch 100 loss: 1.6685491108894348\n",
      " batch 200 loss: 1.6617509996891022\n",
      " batch 300 loss: 1.654809159040451\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.4:\n",
      "Test accuracy 41.3:\n",
      "EPOCH 17:\n",
      " batch 100 loss: 1.6543584644794465\n",
      " batch 200 loss: 1.6779556787014007\n",
      " batch 300 loss: 1.6509519076347352\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.5:\n",
      "Test accuracy 41.699999999999996:\n",
      "EPOCH 18:\n",
      " batch 100 loss: 1.6492896485328674\n",
      " batch 200 loss: 1.6520231199264526\n",
      " batch 300 loss: 1.651444398164749\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.4:\n",
      "Test accuracy 41.8:\n",
      "EPOCH 19:\n",
      " batch 100 loss: 1.6470443356037139\n",
      " batch 200 loss: 1.656793178319931\n",
      " batch 300 loss: 1.654459079504013\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.7:\n",
      "Test accuracy 41.199999999999996:\n",
      "EPOCH 20:\n",
      " batch 100 loss: 1.6575116312503815\n",
      " batch 200 loss: 1.6658351063728332\n",
      " batch 300 loss: 1.64948389172554\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.5:\n",
      "Test accuracy 41.3:\n",
      "EPOCH 21:\n",
      " batch 100 loss: 1.6556888496875763\n",
      " batch 200 loss: 1.6594973146915435\n",
      " batch 300 loss: 1.6471678388118745\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.800000000000004:\n",
      "Test accuracy 41.3:\n",
      "EPOCH 22:\n",
      " batch 100 loss: 1.654035598039627\n",
      " batch 200 loss: 1.638121554851532\n",
      " batch 300 loss: 1.6494931733608247\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.5:\n",
      "Test accuracy 41.3:\n",
      "EPOCH 23:\n",
      " batch 100 loss: 1.6458784079551696\n",
      " batch 200 loss: 1.6500677037239075\n",
      " batch 300 loss: 1.653535133600235\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.9:\n",
      "Test accuracy 41.099999999999994:\n",
      "EPOCH 24:\n",
      " batch 100 loss: 1.6448568201065064\n",
      " batch 200 loss: 1.6381929421424866\n",
      " batch 300 loss: 1.6477278792858123\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 44.800000000000004:\n",
      "Test accuracy 41.3:\n",
      "EPOCH 25:\n",
      " batch 100 loss: 1.6375502955913543\n",
      " batch 200 loss: 1.6310348856449126\n",
      " batch 300 loss: 1.6435911071300506\n",
      "lr 0.01 -> 0.001\n",
      "Train accuracy 45.2:\n",
      "Test accuracy 41.699999999999996:\n",
      "EPOCH 26:\n",
      " batch 100 loss: 1.6261572432518006\n",
      " batch 200 loss: 1.6404776275157928\n",
      " batch 300 loss: 1.6237645184993743\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 27:\n",
      " batch 100 loss: 1.621802295446396\n",
      " batch 200 loss: 1.6181298577785492\n",
      " batch 300 loss: 1.646649054288864\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.8:\n",
      "EPOCH 28:\n",
      " batch 100 loss: 1.6416157948970795\n",
      " batch 200 loss: 1.6301886963844299\n",
      " batch 300 loss: 1.6203949356079101\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 41.8:\n",
      "EPOCH 29:\n",
      " batch 100 loss: 1.6458301532268524\n",
      " batch 200 loss: 1.6177581799030305\n",
      " batch 300 loss: 1.6279314363002777\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 41.8:\n",
      "EPOCH 30:\n",
      " batch 100 loss: 1.6288711953163146\n",
      " batch 200 loss: 1.6324268102645874\n",
      " batch 300 loss: 1.6203221559524537\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 31:\n",
      " batch 100 loss: 1.625379707813263\n",
      " batch 200 loss: 1.6319901478290557\n",
      " batch 300 loss: 1.6366412103176118\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 32:\n",
      " batch 100 loss: 1.631940256357193\n",
      " batch 200 loss: 1.6307413506507873\n",
      " batch 300 loss: 1.6192836344242096\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 33:\n",
      " batch 100 loss: 1.6372270333766936\n",
      " batch 200 loss: 1.6260910475254058\n",
      " batch 300 loss: 1.6239155399799348\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 41.8:\n",
      "EPOCH 34:\n",
      " batch 100 loss: 1.6397633123397828\n",
      " batch 200 loss: 1.6155727577209473\n",
      " batch 300 loss: 1.6455949652194977\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 35:\n",
      " batch 100 loss: 1.6333954000473023\n",
      " batch 200 loss: 1.6296148645877837\n",
      " batch 300 loss: 1.6313729655742646\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 36:\n",
      " batch 100 loss: 1.6271377778053284\n",
      " batch 200 loss: 1.6234382641315461\n",
      " batch 300 loss: 1.6455899143218995\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.4:\n",
      "Test accuracy 41.8:\n",
      "EPOCH 37:\n",
      " batch 100 loss: 1.624714810848236\n",
      " batch 200 loss: 1.6340577185153962\n",
      " batch 300 loss: 1.6230086767673493\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 38:\n",
      " batch 100 loss: 1.6355196237564087\n",
      " batch 200 loss: 1.622364513874054\n",
      " batch 300 loss: 1.631541748046875\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 39:\n",
      " batch 100 loss: 1.6266549503803254\n",
      " batch 200 loss: 1.633956949710846\n",
      " batch 300 loss: 1.6320911645889282\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.8:\n",
      "EPOCH 40:\n",
      " batch 100 loss: 1.6277139234542846\n",
      " batch 200 loss: 1.6252427208423614\n",
      " batch 300 loss: 1.6382224345207215\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 41:\n",
      " batch 100 loss: 1.616888473033905\n",
      " batch 200 loss: 1.629707179069519\n",
      " batch 300 loss: 1.6332723414897918\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 42:\n",
      " batch 100 loss: 1.6223496854305268\n",
      " batch 200 loss: 1.626208518743515\n",
      " batch 300 loss: 1.6213176238536835\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.6:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 43:\n",
      " batch 100 loss: 1.6349936211109162\n",
      " batch 200 loss: 1.6324223005771636\n",
      " batch 300 loss: 1.6251959347724914\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 44:\n",
      " batch 100 loss: 1.6188376176357269\n",
      " batch 200 loss: 1.632833731174469\n",
      " batch 300 loss: 1.6334981429576874\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 45:\n",
      " batch 100 loss: 1.6241433560848235\n",
      " batch 200 loss: 1.6282152473926543\n",
      " batch 300 loss: 1.6432887923717499\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 46:\n",
      " batch 100 loss: 1.6338322007656096\n",
      " batch 200 loss: 1.6244558143615722\n",
      " batch 300 loss: 1.6276189815998077\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.1:\n",
      "EPOCH 47:\n",
      " batch 100 loss: 1.6228050434589385\n",
      " batch 200 loss: 1.6315201485157014\n",
      " batch 300 loss: 1.634427902698517\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.6:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 48:\n",
      " batch 100 loss: 1.619507361650467\n",
      " batch 200 loss: 1.6246487057209016\n",
      " batch 300 loss: 1.6347085320949555\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 49:\n",
      " batch 100 loss: 1.6254243576526641\n",
      " batch 200 loss: 1.625166139602661\n",
      " batch 300 loss: 1.6309172010421753\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 50:\n",
      " batch 100 loss: 1.6274288523197173\n",
      " batch 200 loss: 1.6271222507953644\n",
      " batch 300 loss: 1.6384893453121185\n",
      "lr 0.001 -> 0.0001\n",
      "Train accuracy 45.6:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 51:\n",
      " batch 100 loss: 1.6333578765392303\n",
      " batch 200 loss: 1.6276525390148162\n",
      " batch 300 loss: 1.6254417514801025\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.6:\n",
      "Test accuracy 41.9:\n",
      "EPOCH 52:\n",
      " batch 100 loss: 1.616767431497574\n",
      " batch 200 loss: 1.6318161952495576\n",
      " batch 300 loss: 1.62785001039505\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 53:\n",
      " batch 100 loss: 1.6158680248260497\n",
      " batch 200 loss: 1.6299258434772492\n",
      " batch 300 loss: 1.6253467190265656\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 54:\n",
      " batch 100 loss: 1.6251215374469756\n",
      " batch 200 loss: 1.6191651940345764\n",
      " batch 300 loss: 1.628486692905426\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 55:\n",
      " batch 100 loss: 1.6320239365100861\n",
      " batch 200 loss: 1.6268304407596588\n",
      " batch 300 loss: 1.6179238617420197\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 56:\n",
      " batch 100 loss: 1.6244232869148254\n",
      " batch 200 loss: 1.6289521634578705\n",
      " batch 300 loss: 1.622583831548691\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 57:\n",
      " batch 100 loss: 1.623011667728424\n",
      " batch 200 loss: 1.6259590530395507\n",
      " batch 300 loss: 1.6224800336360932\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 58:\n",
      " batch 100 loss: 1.6300354158878327\n",
      " batch 200 loss: 1.6301003301143646\n",
      " batch 300 loss: 1.6181516218185426\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 59:\n",
      " batch 100 loss: 1.6310587704181672\n",
      " batch 200 loss: 1.6266459476947785\n",
      " batch 300 loss: 1.6279838716983794\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 60:\n",
      " batch 100 loss: 1.6450678086280823\n",
      " batch 200 loss: 1.6154465329647065\n",
      " batch 300 loss: 1.6235178363323213\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 61:\n",
      " batch 100 loss: 1.6261589276790618\n",
      " batch 200 loss: 1.6311361527442931\n",
      " batch 300 loss: 1.6231150865554809\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 62:\n",
      " batch 100 loss: 1.63106161236763\n",
      " batch 200 loss: 1.6203265011310577\n",
      " batch 300 loss: 1.6389069867134094\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 63:\n",
      " batch 100 loss: 1.6187521088123322\n",
      " batch 200 loss: 1.6335277187824249\n",
      " batch 300 loss: 1.6294894886016846\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 64:\n",
      " batch 100 loss: 1.6184237468242646\n",
      " batch 200 loss: 1.6241008627414704\n",
      " batch 300 loss: 1.625731613636017\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 65:\n",
      " batch 100 loss: 1.6306373572349548\n",
      " batch 200 loss: 1.6289222609996796\n",
      " batch 300 loss: 1.6210078382492066\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 66:\n",
      " batch 100 loss: 1.614959532022476\n",
      " batch 200 loss: 1.6357015264034271\n",
      " batch 300 loss: 1.624516544342041\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 67:\n",
      " batch 100 loss: 1.6225373291969298\n",
      " batch 200 loss: 1.6178667974472045\n",
      " batch 300 loss: 1.631051994562149\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 68:\n",
      " batch 100 loss: 1.6231679320335388\n",
      " batch 200 loss: 1.6152582108974456\n",
      " batch 300 loss: 1.6218260765075683\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 69:\n",
      " batch 100 loss: 1.6107406318187714\n",
      " batch 200 loss: 1.632423838376999\n",
      " batch 300 loss: 1.6356625008583068\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 70:\n",
      " batch 100 loss: 1.6227446138858794\n",
      " batch 200 loss: 1.6177256298065186\n",
      " batch 300 loss: 1.626770920753479\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 71:\n",
      " batch 100 loss: 1.632570697069168\n",
      " batch 200 loss: 1.6169517588615419\n",
      " batch 300 loss: 1.6299276471138\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 72:\n",
      " batch 100 loss: 1.6094541096687316\n",
      " batch 200 loss: 1.6349986183643341\n",
      " batch 300 loss: 1.630576343536377\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 73:\n",
      " batch 100 loss: 1.6339522612094879\n",
      " batch 200 loss: 1.6190108239650727\n",
      " batch 300 loss: 1.6263729751110076\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 74:\n",
      " batch 100 loss: 1.6157208895683288\n",
      " batch 200 loss: 1.6291565346717833\n",
      " batch 300 loss: 1.623194353580475\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 75:\n",
      " batch 100 loss: 1.6372946274280549\n",
      " batch 200 loss: 1.6299706280231476\n",
      " batch 300 loss: 1.6219169545173644\n",
      "lr 0.0001 -> 1e-05\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 76:\n",
      " batch 100 loss: 1.6132533550262451\n",
      " batch 200 loss: 1.6311901473999024\n",
      " batch 300 loss: 1.6224843907356261\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 77:\n",
      " batch 100 loss: 1.620120840072632\n",
      " batch 200 loss: 1.6250754737854003\n",
      " batch 300 loss: 1.621715750694275\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 78:\n",
      " batch 100 loss: 1.6186907720565795\n",
      " batch 200 loss: 1.6142809188365936\n",
      " batch 300 loss: 1.6277558958530427\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 79:\n",
      " batch 100 loss: 1.6164686322212218\n",
      " batch 200 loss: 1.6179361569881439\n",
      " batch 300 loss: 1.6324080288410188\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "EPOCH 80:\n",
      " batch 100 loss: 1.6201106572151185\n",
      " batch 200 loss: 1.6115927255153657\n",
      " batch 300 loss: 1.6428077006340027\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 45.5:\n",
      "Test accuracy 42.0:\n",
      "Optimizer Adam:\n",
      "EPOCH 1:\n",
      " batch 100 loss: 2.7572129440307616\n",
      " batch 200 loss: 2.44000093460083\n",
      " batch 300 loss: 2.492218233346939\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 19.8:\n",
      "Test accuracy 18.8:\n",
      "EPOCH 2:\n",
      " batch 100 loss: 2.4353488421440126\n",
      " batch 200 loss: 2.4904785323143006\n",
      " batch 300 loss: 2.5222898435592653\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.7:\n",
      "Test accuracy 16.400000000000002:\n",
      "EPOCH 3:\n",
      " batch 100 loss: 2.3950902605056763\n",
      " batch 200 loss: 2.4822686386108397\n",
      " batch 300 loss: 2.5014212346076965\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 18.5:\n",
      "Test accuracy 16.8:\n",
      "EPOCH 4:\n",
      " batch 100 loss: 2.389598569869995\n",
      " batch 200 loss: 2.436619005203247\n",
      " batch 300 loss: 2.469574693441391\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.4:\n",
      "Test accuracy 15.7:\n",
      "EPOCH 5:\n",
      " batch 100 loss: 2.4297488808631895\n",
      " batch 200 loss: 2.3964850544929504\n",
      " batch 300 loss: 2.4675594830513\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.0:\n",
      "Test accuracy 15.5:\n",
      "EPOCH 6:\n",
      " batch 100 loss: 2.4025739669799804\n",
      " batch 200 loss: 2.388120594024658\n",
      " batch 300 loss: 2.3949153089523314\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.1:\n",
      "Test accuracy 14.000000000000002:\n",
      "EPOCH 7:\n",
      " batch 100 loss: 2.385382866859436\n",
      " batch 200 loss: 2.4085068702697754\n",
      " batch 300 loss: 2.3601958394050597\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.8:\n",
      "Test accuracy 16.0:\n",
      "EPOCH 8:\n",
      " batch 100 loss: 2.339872057437897\n",
      " batch 200 loss: 2.3074871826171877\n",
      " batch 300 loss: 2.328322606086731\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 18.8:\n",
      "Test accuracy 17.1:\n",
      "EPOCH 9:\n",
      " batch 100 loss: 2.286882264614105\n",
      " batch 200 loss: 2.274622757434845\n",
      " batch 300 loss: 2.3037667417526246\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.299999999999997:\n",
      "Test accuracy 15.7:\n",
      "EPOCH 10:\n",
      " batch 100 loss: 2.323067047595978\n",
      " batch 200 loss: 2.3251590824127195\n",
      " batch 300 loss: 2.3201928400993346\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 18.3:\n",
      "Test accuracy 16.400000000000002:\n",
      "EPOCH 11:\n",
      " batch 100 loss: 2.272858052253723\n",
      " batch 200 loss: 2.2672824573516848\n",
      " batch 300 loss: 2.2850100708007814\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.1:\n",
      "Test accuracy 15.4:\n",
      "EPOCH 12:\n",
      " batch 100 loss: 2.3026250910758974\n",
      " batch 200 loss: 2.277277307510376\n",
      " batch 300 loss: 2.2892803525924683\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.1:\n",
      "Test accuracy 14.099999999999998:\n",
      "EPOCH 13:\n",
      " batch 100 loss: 2.244254047870636\n",
      " batch 200 loss: 2.2827694964408876\n",
      " batch 300 loss: 2.2696699905395508\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.1:\n",
      "Test accuracy 13.600000000000001:\n",
      "EPOCH 14:\n",
      " batch 100 loss: 2.237469629049301\n",
      " batch 200 loss: 2.2486298871040344\n",
      " batch 300 loss: 2.3350657391548157\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.9:\n",
      "Test accuracy 16.1:\n",
      "EPOCH 15:\n",
      " batch 100 loss: 2.2270546054840086\n",
      " batch 200 loss: 2.242237524986267\n",
      " batch 300 loss: 2.2612923336029054\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.1:\n",
      "Test accuracy 14.099999999999998:\n",
      "EPOCH 16:\n",
      " batch 100 loss: 2.2641883766651154\n",
      " batch 200 loss: 2.290206558704376\n",
      " batch 300 loss: 2.294796805381775\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.4:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 17:\n",
      " batch 100 loss: 2.2576656675338747\n",
      " batch 200 loss: 2.2409715378284454\n",
      " batch 300 loss: 2.314195466041565\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.3:\n",
      "Test accuracy 14.799999999999999:\n",
      "EPOCH 18:\n",
      " batch 100 loss: 2.2500449204444886\n",
      " batch 200 loss: 2.2731262016296387\n",
      " batch 300 loss: 2.2354423356056214\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.400000000000002:\n",
      "Test accuracy 15.0:\n",
      "EPOCH 19:\n",
      " batch 100 loss: 2.260823540687561\n",
      " batch 200 loss: 2.254952847957611\n",
      " batch 300 loss: 2.292320213317871\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.2:\n",
      "Test accuracy 14.899999999999999:\n",
      "EPOCH 20:\n",
      " batch 100 loss: 2.268479824066162\n",
      " batch 200 loss: 2.2566315078735353\n",
      " batch 300 loss: 2.2604213976860046\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 17.9:\n",
      "Test accuracy 16.0:\n",
      "EPOCH 21:\n",
      " batch 100 loss: 2.2694027161598207\n",
      " batch 200 loss: 2.249628710746765\n",
      " batch 300 loss: 2.264111979007721\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 18.099999999999998:\n",
      "Test accuracy 16.5:\n",
      "EPOCH 22:\n",
      " batch 100 loss: 2.238513481616974\n",
      " batch 200 loss: 2.2551494789123536\n",
      " batch 300 loss: 2.2713767504692077\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.1:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 23:\n",
      " batch 100 loss: 2.271301188468933\n",
      " batch 200 loss: 2.2735905385017396\n",
      " batch 300 loss: 2.248781294822693\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.0:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 24:\n",
      " batch 100 loss: 2.2903334045410157\n",
      " batch 200 loss: 2.243785574436188\n",
      " batch 300 loss: 2.3072741055488586\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.2:\n",
      "Test accuracy 14.7:\n",
      "EPOCH 25:\n",
      " batch 100 loss: 2.2639004588127136\n",
      " batch 200 loss: 2.2601586198806762\n",
      " batch 300 loss: 2.269349660873413\n",
      "lr 0.01 -> 0.001\n",
      "Train accuracy 15.7:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 26:\n",
      " batch 100 loss: 2.1745459616184233\n",
      " batch 200 loss: 2.157747368812561\n",
      " batch 300 loss: 2.150494122505188\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 19.6:\n",
      "Test accuracy 17.299999999999997:\n",
      "EPOCH 27:\n",
      " batch 100 loss: 2.128167716264725\n",
      " batch 200 loss: 2.1152203977108\n",
      " batch 300 loss: 2.1080136597156525\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 19.8:\n",
      "Test accuracy 17.2:\n",
      "EPOCH 28:\n",
      " batch 100 loss: 2.0873900234699247\n",
      " batch 200 loss: 2.1052376580238343\n",
      " batch 300 loss: 2.096954482793808\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 20.599999999999998:\n",
      "Test accuracy 18.0:\n",
      "EPOCH 29:\n",
      " batch 100 loss: 2.084258635044098\n",
      " batch 200 loss: 2.0859828996658325\n",
      " batch 300 loss: 2.087806963920593\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 21.3:\n",
      "Test accuracy 18.6:\n",
      "EPOCH 30:\n",
      " batch 100 loss: 2.061915781497955\n",
      " batch 200 loss: 2.0764271199703215\n",
      " batch 300 loss: 2.089147927761078\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 21.2:\n",
      "Test accuracy 18.2:\n",
      "EPOCH 31:\n",
      " batch 100 loss: 2.0662067842483522\n",
      " batch 200 loss: 2.06404620885849\n",
      " batch 300 loss: 2.062280532121658\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 21.4:\n",
      "Test accuracy 18.5:\n",
      "EPOCH 32:\n",
      " batch 100 loss: 2.0651671326160432\n",
      " batch 200 loss: 2.076906620264053\n",
      " batch 300 loss: 2.053736344575882\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.3:\n",
      "Test accuracy 19.2:\n",
      "EPOCH 33:\n",
      " batch 100 loss: 2.048702211380005\n",
      " batch 200 loss: 2.07071342587471\n",
      " batch 300 loss: 2.041599612236023\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.1:\n",
      "Test accuracy 19.1:\n",
      "EPOCH 34:\n",
      " batch 100 loss: 2.043410166501999\n",
      " batch 200 loss: 2.0558797407150267\n",
      " batch 300 loss: 2.0510396361351013\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.2:\n",
      "Test accuracy 19.0:\n",
      "EPOCH 35:\n",
      " batch 100 loss: 2.0364999496936798\n",
      " batch 200 loss: 2.0445563232898714\n",
      " batch 300 loss: 2.048684792518616\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.7:\n",
      "Test accuracy 19.6:\n",
      "EPOCH 36:\n",
      " batch 100 loss: 2.035116430521011\n",
      " batch 200 loss: 2.047278572320938\n",
      " batch 300 loss: 2.044216402769089\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.5:\n",
      "Test accuracy 19.5:\n",
      "EPOCH 37:\n",
      " batch 100 loss: 2.0188123273849485\n",
      " batch 200 loss: 2.0357627022266387\n",
      " batch 300 loss: 2.031108411550522\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.400000000000002:\n",
      "Test accuracy 19.1:\n",
      "EPOCH 38:\n",
      " batch 100 loss: 2.029443553686142\n",
      " batch 200 loss: 2.031474761962891\n",
      " batch 300 loss: 2.030705757141113\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.900000000000002:\n",
      "Test accuracy 19.8:\n",
      "EPOCH 39:\n",
      " batch 100 loss: 2.0342435908317564\n",
      " batch 200 loss: 2.0333788776397705\n",
      " batch 300 loss: 2.0317331671714784\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.1:\n",
      "Test accuracy 19.900000000000002:\n",
      "EPOCH 40:\n",
      " batch 100 loss: 2.0180024945735933\n",
      " batch 200 loss: 2.012690703868866\n",
      " batch 300 loss: 2.0337887513637543\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 22.7:\n",
      "Test accuracy 19.2:\n",
      "EPOCH 41:\n",
      " batch 100 loss: 2.0212048959732054\n",
      " batch 200 loss: 2.008064634799957\n",
      " batch 300 loss: 2.0263112604618074\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.599999999999998:\n",
      "Test accuracy 20.3:\n",
      "EPOCH 42:\n",
      " batch 100 loss: 2.012852849960327\n",
      " batch 200 loss: 2.029458875656128\n",
      " batch 300 loss: 2.023073444366455\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.599999999999998:\n",
      "Test accuracy 20.4:\n",
      "EPOCH 43:\n",
      " batch 100 loss: 2.00387681722641\n",
      " batch 200 loss: 2.024932463169098\n",
      " batch 300 loss: 2.003742650747299\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.599999999999998:\n",
      "Test accuracy 20.3:\n",
      "EPOCH 44:\n",
      " batch 100 loss: 2.0022896838188173\n",
      " batch 200 loss: 2.003791811466217\n",
      " batch 300 loss: 2.0143582046031954\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.3:\n",
      "Test accuracy 19.5:\n",
      "EPOCH 45:\n",
      " batch 100 loss: 2.008646214008331\n",
      " batch 200 loss: 2.009248824119568\n",
      " batch 300 loss: 2.0008770561218263\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.400000000000002:\n",
      "Test accuracy 19.400000000000002:\n",
      "EPOCH 46:\n",
      " batch 100 loss: 1.9967848706245421\n",
      " batch 200 loss: 2.0150705432891844\n",
      " batch 300 loss: 2.0006748950481414\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.5:\n",
      "Test accuracy 19.6:\n",
      "EPOCH 47:\n",
      " batch 100 loss: 2.0046513092517855\n",
      " batch 200 loss: 2.001957672834396\n",
      " batch 300 loss: 2.0108659684658052\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.599999999999998:\n",
      "Test accuracy 19.7:\n",
      "EPOCH 48:\n",
      " batch 100 loss: 1.9965184891223908\n",
      " batch 200 loss: 1.9982760846614838\n",
      " batch 300 loss: 2.005985769033432\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 23.9:\n",
      "Test accuracy 20.4:\n",
      "EPOCH 49:\n",
      " batch 100 loss: 1.9881494092941283\n",
      " batch 200 loss: 2.0069612646102906\n",
      " batch 300 loss: 2.0026871728897095\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 24.099999999999998:\n",
      "Test accuracy 20.200000000000003:\n",
      "EPOCH 50:\n",
      " batch 100 loss: 1.9919322156906127\n",
      " batch 200 loss: 1.9896567499637603\n",
      " batch 300 loss: 2.008439860343933\n",
      "lr 0.001 -> 0.0001\n",
      "Train accuracy 24.2:\n",
      "Test accuracy 20.3:\n",
      "EPOCH 51:\n",
      " batch 100 loss: 1.9807298982143402\n",
      " batch 200 loss: 1.9637146830558776\n",
      " batch 300 loss: 1.975250631570816\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.4:\n",
      "Test accuracy 20.4:\n",
      "EPOCH 52:\n",
      " batch 100 loss: 1.9717214703559875\n",
      " batch 200 loss: 1.9745144844055176\n",
      " batch 300 loss: 1.9731209671497345\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 53:\n",
      " batch 100 loss: 1.9705973148345948\n",
      " batch 200 loss: 1.9650939631462097\n",
      " batch 300 loss: 1.9639297115802765\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.4:\n",
      "Test accuracy 20.3:\n",
      "EPOCH 54:\n",
      " batch 100 loss: 1.9631292474269868\n",
      " batch 200 loss: 1.9611853468418121\n",
      " batch 300 loss: 1.9693515157699586\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.4:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 55:\n",
      " batch 100 loss: 1.9704066359996795\n",
      " batch 200 loss: 1.967685090303421\n",
      " batch 300 loss: 1.970595237016678\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.4:\n",
      "EPOCH 56:\n",
      " batch 100 loss: 1.9729988062381745\n",
      " batch 200 loss: 1.9589108455181121\n",
      " batch 300 loss: 1.9728483772277832\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 57:\n",
      " batch 100 loss: 1.9629850018024444\n",
      " batch 200 loss: 1.9667965698242187\n",
      " batch 300 loss: 1.9631205272674561\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.4:\n",
      "EPOCH 58:\n",
      " batch 100 loss: 1.9689813697338103\n",
      " batch 200 loss: 1.9662523174285889\n",
      " batch 300 loss: 1.9644723904132844\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 59:\n",
      " batch 100 loss: 1.9578580808639527\n",
      " batch 200 loss: 1.9549179553985596\n",
      " batch 300 loss: 1.9722542619705201\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.7:\n",
      "EPOCH 60:\n",
      " batch 100 loss: 1.975268586874008\n",
      " batch 200 loss: 1.9703880834579468\n",
      " batch 300 loss: 1.9680911302566528\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.7:\n",
      "EPOCH 61:\n",
      " batch 100 loss: 1.9521895217895509\n",
      " batch 200 loss: 1.976449477672577\n",
      " batch 300 loss: 1.975143976211548\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 62:\n",
      " batch 100 loss: 1.963192480802536\n",
      " batch 200 loss: 1.9656674253940583\n",
      " batch 300 loss: 1.968040838241577\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 63:\n",
      " batch 100 loss: 1.9505786907672882\n",
      " batch 200 loss: 1.969885687828064\n",
      " batch 300 loss: 1.9596508944034576\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.5:\n",
      "Test accuracy 20.3:\n",
      "EPOCH 64:\n",
      " batch 100 loss: 1.9739816200733185\n",
      " batch 200 loss: 1.9549267065525056\n",
      " batch 300 loss: 1.9796454906463623\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.7:\n",
      "EPOCH 65:\n",
      " batch 100 loss: 1.959535619020462\n",
      " batch 200 loss: 1.9647003078460694\n",
      " batch 300 loss: 1.9711811316013337\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 66:\n",
      " batch 100 loss: 1.9693252408504487\n",
      " batch 200 loss: 1.9735229218006134\n",
      " batch 300 loss: 1.9555919992923736\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 67:\n",
      " batch 100 loss: 1.9553516495227814\n",
      " batch 200 loss: 1.9721354293823241\n",
      " batch 300 loss: 1.9741868710517883\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 68:\n",
      " batch 100 loss: 1.957893007993698\n",
      " batch 200 loss: 1.9664759540557861\n",
      " batch 300 loss: 1.961065138578415\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.8:\n",
      "EPOCH 69:\n",
      " batch 100 loss: 1.972925180196762\n",
      " batch 200 loss: 1.969067212343216\n",
      " batch 300 loss: 1.9438306975364685\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 70:\n",
      " batch 100 loss: 1.9525032091140746\n",
      " batch 200 loss: 1.9667556667327881\n",
      " batch 300 loss: 1.9614141988754272\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.7:\n",
      "EPOCH 71:\n",
      " batch 100 loss: 1.9548538625240326\n",
      " batch 200 loss: 1.968794902563095\n",
      " batch 300 loss: 1.9573840773105622\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.7:\n",
      "EPOCH 72:\n",
      " batch 100 loss: 1.968286519050598\n",
      " batch 200 loss: 1.9568517327308654\n",
      " batch 300 loss: 1.9705558347702026\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 73:\n",
      " batch 100 loss: 1.95612034201622\n",
      " batch 200 loss: 1.9659341502189636\n",
      " batch 300 loss: 1.9617546057701112\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 74:\n",
      " batch 100 loss: 1.9593428432941438\n",
      " batch 200 loss: 1.957598795890808\n",
      " batch 300 loss: 1.9702532553672791\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 75:\n",
      " batch 100 loss: 1.9498235368728638\n",
      " batch 200 loss: 1.9690267038345337\n",
      " batch 300 loss: 1.9586351215839386\n",
      "lr 0.0001 -> 1e-05\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 76:\n",
      " batch 100 loss: 1.9555780959129334\n",
      " batch 200 loss: 1.9591772890090942\n",
      " batch 300 loss: 1.9556155073642731\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 24.7:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 77:\n",
      " batch 100 loss: 1.955284011363983\n",
      " batch 200 loss: 1.9554259943962098\n",
      " batch 300 loss: 1.9557522320747376\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 78:\n",
      " batch 100 loss: 1.971500691175461\n",
      " batch 200 loss: 1.9450388932228089\n",
      " batch 300 loss: 1.9467740619182587\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 24.7:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 79:\n",
      " batch 100 loss: 1.964975448846817\n",
      " batch 200 loss: 1.948224880695343\n",
      " batch 300 loss: 1.960487220287323\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "EPOCH 80:\n",
      " batch 100 loss: 1.957589441537857\n",
      " batch 200 loss: 1.9573099529743194\n",
      " batch 300 loss: 1.9464854216575622\n",
      "lr 1e-05 -> 1e-05\n",
      "Train accuracy 24.6:\n",
      "Test accuracy 20.599999999999998:\n",
      "Optimizer RMSProp:\n",
      "EPOCH 1:\n",
      " batch 100 loss: 6.085588438510895\n",
      " batch 200 loss: 3.544172224998474\n",
      " batch 300 loss: 3.4689527344703674\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 14.799999999999999:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 2:\n",
      " batch 100 loss: 3.2010227727890013\n",
      " batch 200 loss: 3.1900978827476503\n",
      " batch 300 loss: 3.228225769996643\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 14.000000000000002:\n",
      "Test accuracy 12.2:\n",
      "EPOCH 3:\n",
      " batch 100 loss: 3.1227853202819826\n",
      " batch 200 loss: 3.0188003730773927\n",
      " batch 300 loss: 3.1829311299324035\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.9:\n",
      "Test accuracy 14.799999999999999:\n",
      "EPOCH 4:\n",
      " batch 100 loss: 3.051902813911438\n",
      " batch 200 loss: 3.2044086170196535\n",
      " batch 300 loss: 2.9740491580963133\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 13.600000000000001:\n",
      "Test accuracy 12.6:\n",
      "EPOCH 5:\n",
      " batch 100 loss: 3.0614842128753663\n",
      " batch 200 loss: 3.0266410636901857\n",
      " batch 300 loss: 3.066088764667511\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 14.6:\n",
      "Test accuracy 13.5:\n",
      "EPOCH 6:\n",
      " batch 100 loss: 2.9042793679237366\n",
      " batch 200 loss: 3.0572590684890746\n",
      " batch 300 loss: 3.078689687252045\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.8:\n",
      "Test accuracy 13.5:\n",
      "EPOCH 7:\n",
      " batch 100 loss: 2.997566475868225\n",
      " batch 200 loss: 2.885714843273163\n",
      " batch 300 loss: 2.9939871501922606\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 14.7:\n",
      "Test accuracy 13.5:\n",
      "EPOCH 8:\n",
      " batch 100 loss: 2.9068284106254576\n",
      " batch 200 loss: 3.048461925983429\n",
      " batch 300 loss: 2.9227630615234377\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 14.6:\n",
      "Test accuracy 12.7:\n",
      "EPOCH 9:\n",
      " batch 100 loss: 2.867044427394867\n",
      " batch 200 loss: 2.9918919920921327\n",
      " batch 300 loss: 2.834570026397705\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.6:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 10:\n",
      " batch 100 loss: 2.856110105514526\n",
      " batch 200 loss: 2.8531297779083253\n",
      " batch 300 loss: 3.022139036655426\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 14.499999999999998:\n",
      "Test accuracy 13.100000000000001:\n",
      "EPOCH 11:\n",
      " batch 100 loss: 2.8577486753463743\n",
      " batch 200 loss: 2.8124565863609314\n",
      " batch 300 loss: 2.88838339805603\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.4:\n",
      "Test accuracy 13.700000000000001:\n",
      "EPOCH 12:\n",
      " batch 100 loss: 2.83229887008667\n",
      " batch 200 loss: 2.853988995552063\n",
      " batch 300 loss: 2.9180312609672545\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.0:\n",
      "Test accuracy 14.099999999999998:\n",
      "EPOCH 13:\n",
      " batch 100 loss: 2.741300003528595\n",
      " batch 200 loss: 2.8604459738731385\n",
      " batch 300 loss: 2.832452487945557\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 14.799999999999999:\n",
      "Test accuracy 12.6:\n",
      "EPOCH 14:\n",
      " batch 100 loss: 2.792498083114624\n",
      " batch 200 loss: 2.8416365814208984\n",
      " batch 300 loss: 2.8675989508628845\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.4:\n",
      "Test accuracy 13.200000000000001:\n",
      "EPOCH 15:\n",
      " batch 100 loss: 2.8184791827201843\n",
      " batch 200 loss: 2.711194922924042\n",
      " batch 300 loss: 2.8176354360580445\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.2:\n",
      "Test accuracy 13.200000000000001:\n",
      "EPOCH 16:\n",
      " batch 100 loss: 2.8110641384124757\n",
      " batch 200 loss: 2.8309977722167967\n",
      " batch 300 loss: 2.67823992729187\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.400000000000002:\n",
      "Test accuracy 14.299999999999999:\n",
      "EPOCH 17:\n",
      " batch 100 loss: 2.797779381275177\n",
      " batch 200 loss: 2.7286571717262267\n",
      " batch 300 loss: 2.717525327205658\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.0:\n",
      "Test accuracy 14.7:\n",
      "EPOCH 18:\n",
      " batch 100 loss: 2.700873227119446\n",
      " batch 200 loss: 2.763049919605255\n",
      " batch 300 loss: 2.6938992285728456\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 13.8:\n",
      "Test accuracy 11.899999999999999:\n",
      "EPOCH 19:\n",
      " batch 100 loss: 2.7046839952468873\n",
      " batch 200 loss: 2.6824397826194764\n",
      " batch 300 loss: 2.6990843081474303\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 15.6:\n",
      "Test accuracy 13.4:\n",
      "EPOCH 20:\n",
      " batch 100 loss: 2.678568742275238\n",
      " batch 200 loss: 2.7695400285720826\n",
      " batch 300 loss: 2.686988890171051\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.2:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 21:\n",
      " batch 100 loss: 2.786800501346588\n",
      " batch 200 loss: 2.631284534931183\n",
      " batch 300 loss: 2.6067736530303955\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.5:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 22:\n",
      " batch 100 loss: 2.6113084316253663\n",
      " batch 200 loss: 2.6831536197662356\n",
      " batch 300 loss: 2.718988075256348\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 13.700000000000001:\n",
      "Test accuracy 11.899999999999999:\n",
      "EPOCH 23:\n",
      " batch 100 loss: 2.546643636226654\n",
      " batch 200 loss: 2.73909307718277\n",
      " batch 300 loss: 2.624898452758789\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 13.900000000000002:\n",
      "Test accuracy 12.5:\n",
      "EPOCH 24:\n",
      " batch 100 loss: 2.6634727573394774\n",
      " batch 200 loss: 2.6613053393363955\n",
      " batch 300 loss: 2.650382239818573\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 16.3:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 25:\n",
      " batch 100 loss: 2.5463530015945435\n",
      " batch 200 loss: 2.6838009643554686\n",
      " batch 300 loss: 2.5468396449089052\n",
      "lr 0.01 -> 0.001\n",
      "Train accuracy 15.0:\n",
      "Test accuracy 12.9:\n",
      "EPOCH 26:\n",
      " batch 100 loss: 2.216915167570114\n",
      " batch 200 loss: 2.181336810588837\n",
      " batch 300 loss: 2.1723973155021667\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.2:\n",
      "Test accuracy 14.000000000000002:\n",
      "EPOCH 27:\n",
      " batch 100 loss: 2.144795914888382\n",
      " batch 200 loss: 2.1434641075134278\n",
      " batch 300 loss: 2.152833342552185\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.5:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 28:\n",
      " batch 100 loss: 2.129437470436096\n",
      " batch 200 loss: 2.136696746349335\n",
      " batch 300 loss: 2.139155511856079\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.4:\n",
      "Test accuracy 13.8:\n",
      "EPOCH 29:\n",
      " batch 100 loss: 2.112420617341995\n",
      " batch 200 loss: 2.129071682691574\n",
      " batch 300 loss: 2.1270726001262665\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.5:\n",
      "Test accuracy 14.000000000000002:\n",
      "EPOCH 30:\n",
      " batch 100 loss: 2.1115730595588684\n",
      " batch 200 loss: 2.1221284925937653\n",
      " batch 300 loss: 2.123693732023239\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.5:\n",
      "Test accuracy 13.700000000000001:\n",
      "EPOCH 31:\n",
      " batch 100 loss: 2.1111392199993135\n",
      " batch 200 loss: 2.113066710233688\n",
      " batch 300 loss: 2.114299474954605\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.599999999999998:\n",
      "Test accuracy 13.700000000000001:\n",
      "EPOCH 32:\n",
      " batch 100 loss: 2.087945524454117\n",
      " batch 200 loss: 2.1211055648326873\n",
      " batch 300 loss: 2.1227022910118105\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.9:\n",
      "Test accuracy 14.099999999999998:\n",
      "EPOCH 33:\n",
      " batch 100 loss: 2.1081817138195036\n",
      " batch 200 loss: 2.1119382441043855\n",
      " batch 300 loss: 2.100477223396301\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.0:\n",
      "Test accuracy 14.299999999999999:\n",
      "EPOCH 34:\n",
      " batch 100 loss: 2.0894262957572938\n",
      " batch 200 loss: 2.101850411891937\n",
      " batch 300 loss: 2.1072309648990633\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.7:\n",
      "Test accuracy 13.8:\n",
      "EPOCH 35:\n",
      " batch 100 loss: 2.1010252845287325\n",
      " batch 200 loss: 2.0903525984287263\n",
      " batch 300 loss: 2.099704911708832\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 17.9:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 36:\n",
      " batch 100 loss: 2.0903042650222776\n",
      " batch 200 loss: 2.0916473090648653\n",
      " batch 300 loss: 2.097351230382919\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.4:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 37:\n",
      " batch 100 loss: 2.092335339784622\n",
      " batch 200 loss: 2.0956945359706878\n",
      " batch 300 loss: 2.0827605545520784\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.099999999999998:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 38:\n",
      " batch 100 loss: 2.0806774067878724\n",
      " batch 200 loss: 2.0965323746204376\n",
      " batch 300 loss: 2.0878478026390077\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.099999999999998:\n",
      "Test accuracy 14.299999999999999:\n",
      "EPOCH 39:\n",
      " batch 100 loss: 2.0809097623825075\n",
      " batch 200 loss: 2.0813861727714538\n",
      " batch 300 loss: 2.0929818046092987\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.0:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 40:\n",
      " batch 100 loss: 2.0722982549667357\n",
      " batch 200 loss: 2.0824263453483582\n",
      " batch 300 loss: 2.091499711275101\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.0:\n",
      "Test accuracy 14.099999999999998:\n",
      "EPOCH 41:\n",
      " batch 100 loss: 2.0730114090442657\n",
      " batch 200 loss: 2.0919997870922087\n",
      " batch 300 loss: 2.0843766582012178\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.2:\n",
      "Test accuracy 14.299999999999999:\n",
      "EPOCH 42:\n",
      " batch 100 loss: 2.0855489230155944\n",
      " batch 200 loss: 2.0869858145713804\n",
      " batch 300 loss: 2.081228827238083\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.3:\n",
      "Test accuracy 14.2:\n",
      "EPOCH 43:\n",
      " batch 100 loss: 2.078900936841965\n",
      " batch 200 loss: 2.070702977180481\n",
      " batch 300 loss: 2.0712727344036104\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.099999999999998:\n",
      "Test accuracy 14.000000000000002:\n",
      "EPOCH 44:\n",
      " batch 100 loss: 2.070812684297562\n",
      " batch 200 loss: 2.0685026025772095\n",
      " batch 300 loss: 2.088094961643219\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.5:\n",
      "Test accuracy 14.099999999999998:\n",
      "EPOCH 45:\n",
      " batch 100 loss: 2.0732560777664184\n",
      " batch 200 loss: 2.0699881994724274\n",
      " batch 300 loss: 2.0912252748012543\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.5:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 46:\n",
      " batch 100 loss: 2.0778882730007173\n",
      " batch 200 loss: 2.072525409460068\n",
      " batch 300 loss: 2.0810534524917603\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.7:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 47:\n",
      " batch 100 loss: 2.060720542669296\n",
      " batch 200 loss: 2.075736825466156\n",
      " batch 300 loss: 2.0824201095104216\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.8:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 48:\n",
      " batch 100 loss: 2.071892421245575\n",
      " batch 200 loss: 2.067226600646973\n",
      " batch 300 loss: 2.0633875620365143\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.6:\n",
      "Test accuracy 14.7:\n",
      "EPOCH 49:\n",
      " batch 100 loss: 2.069604250192642\n",
      " batch 200 loss: 2.0710058879852293\n",
      " batch 300 loss: 2.083797900676727\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 18.9:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 50:\n",
      " batch 100 loss: 2.05829788684845\n",
      " batch 200 loss: 2.0638681542873383\n",
      " batch 300 loss: 2.0651805114746096\n",
      "lr 0.001 -> 0.0001\n",
      "Train accuracy 18.4:\n",
      "Test accuracy 13.8:\n",
      "EPOCH 51:\n",
      " batch 100 loss: 2.0680960428714754\n",
      " batch 200 loss: 2.0392743694782256\n",
      " batch 300 loss: 2.03479128241539\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.2:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 52:\n",
      " batch 100 loss: 2.047981637716293\n",
      " batch 200 loss: 2.0391979956626893\n",
      " batch 300 loss: 2.0456509554386137\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 53:\n",
      " batch 100 loss: 2.036575824022293\n",
      " batch 200 loss: 2.048691564798355\n",
      " batch 300 loss: 2.043222892284393\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 54:\n",
      " batch 100 loss: 2.048850942850113\n",
      " batch 200 loss: 2.0349553847312927\n",
      " batch 300 loss: 2.044694799184799\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.2:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 55:\n",
      " batch 100 loss: 2.0355424129962922\n",
      " batch 200 loss: 2.042994393110275\n",
      " batch 300 loss: 2.0517175900936127\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 56:\n",
      " batch 100 loss: 2.038174779415131\n",
      " batch 200 loss: 2.049600256681442\n",
      " batch 300 loss: 2.038254820108414\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 57:\n",
      " batch 100 loss: 2.044127011299133\n",
      " batch 200 loss: 2.03951381444931\n",
      " batch 300 loss: 2.0408513569831848\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 58:\n",
      " batch 100 loss: 2.0322202014923096\n",
      " batch 200 loss: 2.0465673446655273\n",
      " batch 300 loss: 2.036889579296112\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 59:\n",
      " batch 100 loss: 2.0391838788986205\n",
      " batch 200 loss: 2.042788143157959\n",
      " batch 300 loss: 2.0493488550186156\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.7:\n",
      "EPOCH 60:\n",
      " batch 100 loss: 2.0531950438022615\n",
      " batch 200 loss: 2.037508417367935\n",
      " batch 300 loss: 2.0370319724082946\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 61:\n",
      " batch 100 loss: 2.034169750213623\n",
      " batch 200 loss: 2.049917604923248\n",
      " batch 300 loss: 2.0446825003623963\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 62:\n",
      " batch 100 loss: 2.0331121814250945\n",
      " batch 200 loss: 2.044673249721527\n",
      " batch 300 loss: 2.0471065402030946\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 63:\n",
      " batch 100 loss: 2.035782814025879\n",
      " batch 200 loss: 2.0389403200149534\n",
      " batch 300 loss: 2.0371796917915344\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 64:\n",
      " batch 100 loss: 2.0446133971214295\n",
      " batch 200 loss: 2.0367970383167266\n",
      " batch 300 loss: 2.0442285537719727\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 65:\n",
      " batch 100 loss: 2.0314996492862702\n",
      " batch 200 loss: 2.0395242965221403\n",
      " batch 300 loss: 2.042338042259216\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 66:\n",
      " batch 100 loss: 2.042053592205048\n",
      " batch 200 loss: 2.044478269815445\n",
      " batch 300 loss: 2.032118854522705\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 67:\n",
      " batch 100 loss: 2.0340451800823214\n",
      " batch 200 loss: 2.040594871044159\n",
      " batch 300 loss: 2.0364132177829743\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.400000000000002:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 68:\n",
      " batch 100 loss: 2.04975835442543\n",
      " batch 200 loss: 2.044367011785507\n",
      " batch 300 loss: 2.0273639810085298\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.399999999999999:\n",
      "EPOCH 69:\n",
      " batch 100 loss: 2.04431805729866\n",
      " batch 200 loss: 2.038098452091217\n",
      " batch 300 loss: 2.03493371963501\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.3:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 70:\n",
      " batch 100 loss: 2.0475446820259093\n",
      " batch 200 loss: 2.036927658319473\n",
      " batch 300 loss: 2.023020086288452\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.2:\n",
      "Test accuracy 14.299999999999999:\n",
      "EPOCH 71:\n",
      " batch 100 loss: 2.048088071346283\n",
      " batch 200 loss: 2.034501496553421\n",
      " batch 300 loss: 2.0343193829059603\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.400000000000002:\n",
      "Test accuracy 14.6:\n",
      "EPOCH 72:\n",
      " batch 100 loss: 2.0251159834861756\n",
      " batch 200 loss: 2.042322726249695\n",
      " batch 300 loss: 2.0383384728431704\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.400000000000002:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 73:\n",
      " batch 100 loss: 2.0445774364471436\n",
      " batch 200 loss: 2.0411165988445283\n",
      " batch 300 loss: 2.025944187641144\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.400000000000002:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 74:\n",
      " batch 100 loss: 2.0363364791870118\n",
      " batch 200 loss: 2.043074735403061\n",
      " batch 300 loss: 2.041488757133484\n",
      "lr 0.0001 -> 0.0001\n",
      "Train accuracy 19.400000000000002:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 75:\n",
      " batch 100 loss: 2.0341258633136747\n",
      " batch 200 loss: 2.034974676370621\n",
      " batch 300 loss: 2.0459066116809845\n",
      "lr 0.0001 -> 1e-05\n",
      "Train accuracy 19.400000000000002:\n",
      "Test accuracy 14.499999999999998:\n",
      "EPOCH 76:\n",
      " batch 100 loss: 2.037631357908249\n",
      " batch 200 loss: 2.0203818917274474\n",
      " batch 300 loss: 2.0345785915851593\n"
     ]
    }
   ],
   "source": [
    "learning_rates = 0.01\n",
    "l2reg = 1e-5\n",
    "description, losses_arr, train_accuracy_arr, test_accuracy_arr = train_with_optimizers(learning_rates,l2reg)\n",
    "save_to_csv('Optimizers', description, losses_arr, train_accuracy_arr, test_accuracy_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTcpDV4IV1zs"
   },
   "source": [
    "### Plotting- lr=0.01, bs=128, l2reg=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHCVdQEfp_Ok"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epochs_arr = np.arange(0, len(losses_arr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhiuGF0PZaFO"
   },
   "outputs": [],
   "source": [
    "# Save the array to a text file (CSV format)\n",
    "np.savetxt('step_data_loss.csv', data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXEegDFVppvI"
   },
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "x = np.array(epochs_arr)\n",
    "for i,train_loss in enumerate(losses_arr):\n",
    "  ypoints = np.array(train_loss)\n",
    "  plt.plot(x, ypoints, label = name[i])\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"train Loss\")\n",
    "plt.legend()\n",
    "plt.title('LR=0.01, L2Reg=1e-5, Step, Batch size=128')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6es2tlnprwR"
   },
   "outputs": [],
   "source": [
    "# Plot training accuracy\n",
    "x = np.array(epochs_arr)\n",
    "for i,train_acc in enumerate(train_accuracy_arr):\n",
    "  ypoints = np.array(train_acc)\n",
    "  plt.plot(x, ypoints, label = name[i])\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"train Accuracy\")\n",
    "plt.legend()\n",
    "plt.title('LR=0.01, L2Reg=1e-5, Step, Batch size=128')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMzk9B3_ptZu"
   },
   "outputs": [],
   "source": [
    "# Plot test accuracy\n",
    "x = np.array(epochs_arr)\n",
    "for i,test_acc in enumerate(test_accuracy_arr):\n",
    "  ypoints = np.array(test_acc)\n",
    "  plt.plot(x, ypoints, label = name[i])\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "plt.title('LR=0.01, L2Reg=1e-5, Step, Batch size=128')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yOmqZP71qtn"
   },
   "outputs": [],
   "source": [
    "# Plot test and training accuracy\n",
    "x = np.array(epochs_arr)\n",
    "for i,lt in enumerate([0,1]):\n",
    "  ypoints_1 = np.array(train_accuracy_arr[i])\n",
    "  ypoints_2 = np.array(test_accuracy_arr[i])\n",
    "  plt.plot(x, ypoints_1, label = 'Train')\n",
    "  plt.plot(x, ypoints_2, label = 'Test')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.legend()\n",
    "  plt.title(F'LR=0.01, Batch_size=128, L2reg=1e-5, Optimizer={name[i]}')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB38SwpjVkx3"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34bzbZtggYsa"
   },
   "outputs": [],
   "source": [
    "model_save_name = '1695168105559.8071_Step_bs128_e30_SGD_ss10_gamma0.1_cifar'\n",
    "path = F\"/content/drive/MyDrive/Ell409/CIFAR/scheduler/{model_save_name}.pt\"\n",
    "net.load_state_dict(t.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT-_48gpVXe9"
   },
   "outputs": [],
   "source": [
    "test(net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJWBQrSQVfay"
   },
   "outputs": [],
   "source": [
    "#hello kuch likhlo warna inactivity timeout de dega yeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PySDxtrTRW_J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMnB1cEKwivNoYwEd1U0ZPr",
   "name": "",
   "provenance": [
    {
     "file_id": "1wGNwoHW3xVOGZBK4XCUdRJB0yqSrAR9-",
     "timestamp": 1695161091996
    },
    {
     "file_id": "15Lmix6u1_Upz2o1ZaphkmkuAmx1gQHy-",
     "timestamp": 1694976575982
    },
    {
     "file_id": "1-6ojp9bMjhfGw9ZcZRZlG8CKheYw_toh",
     "timestamp": 1694894618382
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
