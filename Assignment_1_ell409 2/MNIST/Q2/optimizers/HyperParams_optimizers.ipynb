{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5022,
     "status": "ok",
     "timestamp": 1695240476843,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "_btPg74EYNTh"
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri-Fm4O4VnDc"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695240476844,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "QcHzUTVnegru"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img): #convert + flatten\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQXUMoe0VpVz"
   },
   "source": [
    "\n",
    "\n",
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695240476844,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "SkaJoTYgamtY"
   },
   "outputs": [],
   "source": [
    "num_epochs=50\n",
    "after_every=100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7298,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "GJXtKqjIYUxl",
    "outputId": "ce115be3-93db-4097-a439-4efe96f48031"
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = t.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = t.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONzNfpu3VrPs"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "xMN-3GHIX-AE"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, epoch_index, optimizer, train_dataset):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        # Every data instance is an input + label pair\n",
    "        # data.shape=torch.Size([10, 1, 28, 28]) --> 10 images,labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs.view(-1, 28*28))\n",
    "\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % after_every == after_every-1:\n",
    "            last_loss = running_loss / after_every # loss per after_every batches\n",
    "            print(' batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkhUdxuTVtQ2"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "PaZP0GgJo_7Y"
   },
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "  total=0\n",
    "  correct=0\n",
    "  model.eval()\n",
    "  with t.no_grad():\n",
    "      for data in loader:\n",
    "          x, y = data\n",
    "          output = model(x.view(-1, 28*28))\n",
    "          for idx, i in enumerate(output):\n",
    "              if t.argmax(i) == y[idx]:\n",
    "                  correct +=1\n",
    "              total +=1\n",
    "  return round(correct/total, 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695240484136,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "qAaUlOcloXzr"
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dataset, test_dataset, optimizer, scheduler, LR_type):\n",
    "  epoch_number = 0\n",
    "\n",
    "  losses = []\n",
    "  train_accuracy = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "      # Make sure gradient tracking is on, and do a pass over the data\n",
    "      model.train(True)\n",
    "      avg_loss = train_one_epoch(model, epoch_number,optimizer,train_dataset)\n",
    "      losses.append(avg_loss)\n",
    "\n",
    "      # Train accuracy\n",
    "      train_acc = test(model, train_dataset)\n",
    "      train_accuracy.append(train_acc)\n",
    "\n",
    "      # Test accuracy\n",
    "      test_acc = test(model, test_dataset)\n",
    "      test_accuracy.append(test_acc)\n",
    "\n",
    "      if LR_type=='Plateau':\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step(avg_loss)\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print('lr {} -> {}'.format(before_lr, after_lr))\n",
    "\n",
    "      elif LR_type!='None':\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print('lr {} -> {}'.format(before_lr, after_lr))\n",
    "\n",
    "      print('Train accuracy {}:'.format(train_acc))\n",
    "      print('Test accuracy {}:'.format(test_acc))\n",
    "      epoch_number += 1\n",
    "  return losses, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25800,
     "status": "ok",
     "timestamp": 1695240509932,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "J-gvG1N1o46t",
    "outputId": "4e015ccc-6be8-4b74-a3d8-f5e929d4fffb"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "name = ['SGD', 'Adam', 'RMSProp']\n",
    "def train_with_optimizers(starting_lr, l2reg):\n",
    "  losses_arr = []\n",
    "  train_accuracy_arr = []\n",
    "  test_accuracy_arr = []\n",
    "  description = []\n",
    "  # Different Optimizers\n",
    "  for i,opti in enumerate(name):\n",
    "    print('Optimizer {}:'.format(opti))\n",
    "    net_new = Net()\n",
    "    optimizerr = opti\n",
    "    if(opti=='SGD'):\n",
    "        optimizerr = t.optim.SGD(net_new.parameters(), lr=starting_lr, weight_decay=l2reg)\n",
    "    if(opti=='Adam'):\n",
    "        optimizerr = t.optim.Adam(net_new.parameters(), lr=starting_lr, weight_decay=l2reg)\n",
    "    if(opti=='RMSProp'):\n",
    "        optimizerr = t.optim.RMSprop(net_new.parameters(), lr=starting_lr, weight_decay=l2reg)\n",
    "    scheduler = StepLR(optimizerr, 25, gamma=0.1)\n",
    "\n",
    "    losses, train_accuracy, test_accuracy = train_model(net_new, num_epochs, train_loader, test_loader, optimizerr, scheduler, 'Step')\n",
    "\n",
    "    losses_arr.append(losses)\n",
    "    train_accuracy_arr.append(train_accuracy)\n",
    "    test_accuracy_arr.append(test_accuracy)\n",
    "    description.append('Optimizer: {}, Starting LR:{}, L2Reg:{}, Batch size: {}'.format(name[i], starting_lr, l2reg, batch_size))\n",
    "\n",
    "    # save model\n",
    "    ts_now = dt.datetime.now()\n",
    "    unix_timestamp = dt.datetime.timestamp(ts_now)*1000\n",
    "    model_save_name = str(unix_timestamp)\n",
    "    path = F\"{model_save_name}_{name[i]}_L2Reg={l2reg}_bs{batch_size}_e{num_epochs}.pt\"\n",
    "    t.save(net.state_dict(), path)\n",
    "  return description, losses_arr, train_accuracy_arr, test_accuracy_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695240509932,
     "user": {
      "displayName": "Etiksha Jain",
      "userId": "14034236754514632156"
     },
     "user_tz": -330
    },
    "id": "8OGRXGLja8tY"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def save_to_csv(filename, description, losses_arr, train_acc_arr, test_acc_arr):\n",
    "  # Create a CSV file and write data to it\n",
    "  with open(filename+'.csv', 'w', newline='') as csvfile:\n",
    "    for i, desc in enumerate(description):\n",
    "      fieldnames = ['Description', 'epoch', 'train_loss', 'train_accuracy', 'test_accuracy']\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "      writer.writeheader()\n",
    "      for epoch, (loss, train, test) in enumerate(zip(losses_arr[i], train_acc_arr[i], test_acc_arr[i]), 1):\n",
    "          writer.writerow({'Description':description[i], 'epoch': epoch, 'train_loss': loss, 'train_accuracy': train, 'test_accuracy': test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l8SfkkYWymW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer SGD:\n",
      "EPOCH 1:\n",
      " batch 100 loss: 1.8150903487205505\n",
      " batch 200 loss: 1.3337183940410613\n",
      " batch 300 loss: 1.2030242562294007\n",
      " batch 400 loss: 1.127565987110138\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 63.3:\n",
      "Test accuracy 63.800000000000004:\n",
      "EPOCH 2:\n",
      " batch 100 loss: 1.0305333179235459\n",
      " batch 200 loss: 0.9471209472417832\n",
      " batch 300 loss: 0.9196749871969223\n",
      " batch 400 loss: 0.8973283874988556\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 70.8:\n",
      "Test accuracy 71.1:\n",
      "EPOCH 3:\n",
      " batch 100 loss: 0.8728197151422501\n",
      " batch 200 loss: 0.866802961230278\n",
      " batch 300 loss: 0.8557087063789368\n",
      " batch 400 loss: 0.8331390446424485\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 71.3:\n",
      "Test accuracy 71.5:\n",
      "EPOCH 4:\n",
      " batch 100 loss: 0.8410170811414719\n",
      " batch 200 loss: 0.8116308754682541\n",
      " batch 300 loss: 0.8235679805278778\n",
      " batch 400 loss: 0.8323886543512344\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 71.7:\n",
      "Test accuracy 71.8:\n",
      "EPOCH 5:\n",
      " batch 100 loss: 0.7986624300479889\n",
      " batch 200 loss: 0.8358564710617066\n",
      " batch 300 loss: 0.8008440059423446\n",
      " batch 400 loss: 0.7939886605739593\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 71.89999999999999:\n",
      "Test accuracy 72.1:\n",
      "EPOCH 6:\n",
      " batch 100 loss: 0.7912520438432693\n",
      " batch 200 loss: 0.8109214520454406\n",
      " batch 300 loss: 0.78354955971241\n",
      " batch 400 loss: 0.7924426877498627\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.0:\n",
      "Test accuracy 72.0:\n",
      "EPOCH 7:\n",
      " batch 100 loss: 0.7841144788265229\n",
      " batch 200 loss: 0.7867249399423599\n",
      " batch 300 loss: 0.7870373469591141\n",
      " batch 400 loss: 0.7767140066623688\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.2:\n",
      "Test accuracy 72.39999999999999:\n",
      "EPOCH 8:\n",
      " batch 100 loss: 0.7871818470954896\n",
      " batch 200 loss: 0.7759091490507126\n",
      " batch 300 loss: 0.774059488773346\n",
      " batch 400 loss: 0.7667750942707062\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.2:\n",
      "Test accuracy 72.39999999999999:\n",
      "EPOCH 9:\n",
      " batch 100 loss: 0.7761210560798645\n",
      " batch 200 loss: 0.7661049997806549\n",
      " batch 300 loss: 0.7705916821956634\n",
      " batch 400 loss: 0.761947619318962\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.39999999999999:\n",
      "Test accuracy 72.5:\n",
      "EPOCH 10:\n",
      " batch 100 loss: 0.7563905388116836\n",
      " batch 200 loss: 0.7692056518793106\n",
      " batch 300 loss: 0.7774396061897277\n",
      " batch 400 loss: 0.7604092770814895\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.39999999999999:\n",
      "Test accuracy 72.6:\n",
      "EPOCH 11:\n",
      " batch 100 loss: 0.76723896920681\n",
      " batch 200 loss: 0.7544341540336609\n",
      " batch 300 loss: 0.7712869691848755\n",
      " batch 400 loss: 0.7602084863185883\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.39999999999999:\n",
      "Test accuracy 72.6:\n",
      "EPOCH 12:\n",
      " batch 100 loss: 0.7612833544611931\n",
      " batch 200 loss: 0.7683744966983795\n",
      " batch 300 loss: 0.7684562546014786\n",
      " batch 400 loss: 0.73772895693779\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.6:\n",
      "Test accuracy 72.7:\n",
      "EPOCH 13:\n",
      " batch 100 loss: 0.7620513427257538\n",
      " batch 200 loss: 0.743956960439682\n",
      " batch 300 loss: 0.758205880522728\n",
      " batch 400 loss: 0.7446634691953659\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.7:\n",
      "Test accuracy 72.6:\n",
      "EPOCH 14:\n",
      " batch 100 loss: 0.7618820297718049\n",
      " batch 200 loss: 0.759739979505539\n",
      " batch 300 loss: 0.7527676129341125\n",
      " batch 400 loss: 0.7377202928066253\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.7:\n",
      "Test accuracy 72.7:\n",
      "EPOCH 15:\n",
      " batch 100 loss: 0.747464171051979\n",
      " batch 200 loss: 0.7589568123221397\n",
      " batch 300 loss: 0.7526138007640839\n",
      " batch 400 loss: 0.7401253950595855\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.7:\n",
      "Test accuracy 72.6:\n",
      "EPOCH 16:\n",
      " batch 100 loss: 0.7434384644031524\n",
      " batch 200 loss: 0.7440022233128548\n",
      " batch 300 loss: 0.7439265656471252\n",
      " batch 400 loss: 0.7532624375820159\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.7:\n",
      "Test accuracy 72.7:\n",
      "EPOCH 17:\n",
      " batch 100 loss: 0.7435513269901276\n",
      " batch 200 loss: 0.7471681860089302\n",
      " batch 300 loss: 0.755451670885086\n",
      " batch 400 loss: 0.7342914825677872\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.8:\n",
      "Test accuracy 72.8:\n",
      "EPOCH 18:\n",
      " batch 100 loss: 0.7485250514745713\n",
      " batch 200 loss: 0.7348115462064743\n",
      " batch 300 loss: 0.7478681826591491\n",
      " batch 400 loss: 0.7425098651647568\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.89999999999999:\n",
      "Test accuracy 72.8:\n",
      "EPOCH 19:\n",
      " batch 100 loss: 0.7414381432533265\n",
      " batch 200 loss: 0.7435674285888672\n",
      " batch 300 loss: 0.7446340969204903\n",
      " batch 400 loss: 0.7433749920129776\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 72.89999999999999:\n",
      "Test accuracy 72.8:\n",
      "EPOCH 20:\n",
      " batch 100 loss: 0.7430660235881805\n",
      " batch 200 loss: 0.7438001048564911\n",
      " batch 300 loss: 0.7428471738100052\n",
      " batch 400 loss: 0.735976808667183\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 73.0:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 21:\n",
      " batch 100 loss: 0.730526607632637\n",
      " batch 200 loss: 0.7341334092617035\n",
      " batch 300 loss: 0.7455282771587372\n",
      " batch 400 loss: 0.7386750835180282\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 73.0:\n",
      "Test accuracy 72.7:\n",
      "EPOCH 22:\n",
      " batch 100 loss: 0.7354792141914368\n",
      " batch 200 loss: 0.7319367665052414\n",
      " batch 300 loss: 0.7418392130732536\n",
      " batch 400 loss: 0.7313501936197281\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 73.1:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 23:\n",
      " batch 100 loss: 0.7275846615433693\n",
      " batch 200 loss: 0.7258084133267403\n",
      " batch 300 loss: 0.7433924996852874\n",
      " batch 400 loss: 0.7448002606630325\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 73.1:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 24:\n",
      " batch 100 loss: 0.7344100874662399\n",
      " batch 200 loss: 0.7169335198402405\n",
      " batch 300 loss: 0.7531980484724045\n",
      " batch 400 loss: 0.7250640624761582\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 73.1:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 25:\n",
      " batch 100 loss: 0.7256460881233215\n",
      " batch 200 loss: 0.7265482786297798\n",
      " batch 300 loss: 0.7303049999475479\n",
      " batch 400 loss: 0.7334548017382622\n",
      "lr 0.01 -> 0.001\n",
      "Train accuracy 73.1:\n",
      "Test accuracy 72.7:\n",
      "EPOCH 26:\n",
      " batch 100 loss: 0.7321277302503586\n",
      " batch 200 loss: 0.730946923494339\n",
      " batch 300 loss: 0.7407200032472611\n",
      " batch 400 loss: 0.7287659180164338\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.1:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 27:\n",
      " batch 100 loss: 0.7310354274511337\n",
      " batch 200 loss: 0.7299388748407364\n",
      " batch 300 loss: 0.732117332816124\n",
      " batch 400 loss: 0.7310519254207611\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 28:\n",
      " batch 100 loss: 0.737447282075882\n",
      " batch 200 loss: 0.7308640691637993\n",
      " batch 300 loss: 0.7189262786507606\n",
      " batch 400 loss: 0.7330619060993194\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 29:\n",
      " batch 100 loss: 0.7308659100532532\n",
      " batch 200 loss: 0.7338750943541527\n",
      " batch 300 loss: 0.7214490324258804\n",
      " batch 400 loss: 0.7451488131284714\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 30:\n",
      " batch 100 loss: 0.7330699372291565\n",
      " batch 200 loss: 0.7217159599065781\n",
      " batch 300 loss: 0.7368148374557495\n",
      " batch 400 loss: 0.7381328898668289\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 31:\n",
      " batch 100 loss: 0.7280569186806679\n",
      " batch 200 loss: 0.7331072896718979\n",
      " batch 300 loss: 0.7280518668889999\n",
      " batch 400 loss: 0.7293856358528137\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 32:\n",
      " batch 100 loss: 0.7366927498579026\n",
      " batch 200 loss: 0.7381233292818069\n",
      " batch 300 loss: 0.729161126613617\n",
      " batch 400 loss: 0.7301334568858147\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 33:\n",
      " batch 100 loss: 0.7315652364492417\n",
      " batch 200 loss: 0.72280009329319\n",
      " batch 300 loss: 0.734993358552456\n",
      " batch 400 loss: 0.7320662921667099\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 34:\n",
      " batch 100 loss: 0.7270363116264343\n",
      " batch 200 loss: 0.7364124810695648\n",
      " batch 300 loss: 0.7343567007780075\n",
      " batch 400 loss: 0.7178147235512733\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.8:\n",
      "EPOCH 35:\n",
      " batch 100 loss: 0.7305726659297943\n",
      " batch 200 loss: 0.7385695105791092\n",
      " batch 300 loss: 0.7196240264177323\n",
      " batch 400 loss: 0.7290489774942398\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 36:\n",
      " batch 100 loss: 0.7385703802108765\n",
      " batch 200 loss: 0.7270690879225731\n",
      " batch 300 loss: 0.7187436065077781\n",
      " batch 400 loss: 0.7316808179020882\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 37:\n",
      " batch 100 loss: 0.733398784995079\n",
      " batch 200 loss: 0.7428992295265198\n",
      " batch 300 loss: 0.7317040249705314\n",
      " batch 400 loss: 0.7249813318252564\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 38:\n",
      " batch 100 loss: 0.7205426982045173\n",
      " batch 200 loss: 0.7400147616863251\n",
      " batch 300 loss: 0.7246906161308289\n",
      " batch 400 loss: 0.732230253815651\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 39:\n",
      " batch 100 loss: 0.7255884277820587\n",
      " batch 200 loss: 0.7184591940045357\n",
      " batch 300 loss: 0.7295221012830734\n",
      " batch 400 loss: 0.7442233687639237\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 40:\n",
      " batch 100 loss: 0.7411521422863007\n",
      " batch 200 loss: 0.7241500514745712\n",
      " batch 300 loss: 0.7220451205968856\n",
      " batch 400 loss: 0.7280618864297866\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 41:\n",
      " batch 100 loss: 0.7292787975072861\n",
      " batch 200 loss: 0.7366506904363632\n",
      " batch 300 loss: 0.7236913388967514\n",
      " batch 400 loss: 0.7201336628198624\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 42:\n",
      " batch 100 loss: 0.7358567553758621\n",
      " batch 200 loss: 0.7169346380233764\n",
      " batch 300 loss: 0.7428664422035217\n",
      " batch 400 loss: 0.7211207216978073\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 43:\n",
      " batch 100 loss: 0.731333447098732\n",
      " batch 200 loss: 0.7288762113451958\n",
      " batch 300 loss: 0.7276226451992989\n",
      " batch 400 loss: 0.7206694334745407\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 44:\n",
      " batch 100 loss: 0.720117775797844\n",
      " batch 200 loss: 0.7334114611148834\n",
      " batch 300 loss: 0.7255216619372368\n",
      " batch 400 loss: 0.7362782442569733\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 45:\n",
      " batch 100 loss: 0.7348976546525955\n",
      " batch 200 loss: 0.7393949675559998\n",
      " batch 300 loss: 0.7164628708362579\n",
      " batch 400 loss: 0.7303676855564117\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 46:\n",
      " batch 100 loss: 0.7342430460453033\n",
      " batch 200 loss: 0.7132121247053146\n",
      " batch 300 loss: 0.7447507059574128\n",
      " batch 400 loss: 0.7252284482121467\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 47:\n",
      " batch 100 loss: 0.7298278367519379\n",
      " batch 200 loss: 0.7354924231767654\n",
      " batch 300 loss: 0.7248858875036239\n",
      " batch 400 loss: 0.7340877014398575\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 73.0:\n",
      "EPOCH 48:\n",
      " batch 100 loss: 0.7217514908313751\n",
      " batch 200 loss: 0.7264452749490737\n",
      " batch 300 loss: 0.7418254822492599\n",
      " batch 400 loss: 0.7370026677846908\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 72.89999999999999:\n",
      "EPOCH 49:\n",
      " batch 100 loss: 0.7314004385471344\n",
      " batch 200 loss: 0.7281248962879181\n",
      " batch 300 loss: 0.7310349944233895\n",
      " batch 400 loss: 0.7353605473041535\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 73.0:\n",
      "EPOCH 50:\n",
      " batch 100 loss: 0.7401247763633728\n",
      " batch 200 loss: 0.7237616693973541\n",
      " batch 300 loss: 0.7225076240301133\n",
      " batch 400 loss: 0.7238063383102417\n",
      "lr 0.001 -> 0.0001\n",
      "Train accuracy 73.2:\n",
      "Test accuracy 73.0:\n",
      "Optimizer Adam:\n",
      "EPOCH 1:\n",
      " batch 100 loss: 2.196469051837921\n",
      " batch 200 loss: 1.9522879219055176\n",
      " batch 300 loss: 1.8061867940425873\n",
      " batch 400 loss: 1.8011407709121705\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.3:\n",
      "Test accuracy 36.5:\n",
      "EPOCH 2:\n",
      " batch 100 loss: 1.7953644168376923\n",
      " batch 200 loss: 1.7875514447689056\n",
      " batch 300 loss: 1.771013777256012\n",
      " batch 400 loss: 1.7761025488376618\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.699999999999996:\n",
      "Test accuracy 36.199999999999996:\n",
      "EPOCH 3:\n",
      " batch 100 loss: 1.7962633609771728\n",
      " batch 200 loss: 1.7746745789051055\n",
      " batch 300 loss: 1.7749589109420776\n",
      " batch 400 loss: 1.7930340838432313\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.8:\n",
      "Test accuracy 36.199999999999996:\n",
      "EPOCH 4:\n",
      " batch 100 loss: 1.7800308501720428\n",
      " batch 200 loss: 1.772606248855591\n",
      " batch 300 loss: 1.7958538663387298\n",
      " batch 400 loss: 1.7762572479248047\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.199999999999996:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 5:\n",
      " batch 100 loss: 1.7855350708961486\n",
      " batch 200 loss: 1.7814975607395171\n",
      " batch 300 loss: 1.7774792098999024\n",
      " batch 400 loss: 1.7755347526073455\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.6:\n",
      "Test accuracy 37.0:\n",
      "EPOCH 6:\n",
      " batch 100 loss: 1.7824831485748291\n",
      " batch 200 loss: 1.778810224533081\n",
      " batch 300 loss: 1.8098590672016144\n",
      " batch 400 loss: 1.77598175406456\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.8:\n",
      "Test accuracy 36.199999999999996:\n",
      "EPOCH 7:\n",
      " batch 100 loss: 1.7935469496250152\n",
      " batch 200 loss: 1.7600244891643524\n",
      " batch 300 loss: 1.778307811021805\n",
      " batch 400 loss: 1.788656610250473\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.3:\n",
      "Test accuracy 36.8:\n",
      "EPOCH 8:\n",
      " batch 100 loss: 1.785471384525299\n",
      " batch 200 loss: 1.788356682062149\n",
      " batch 300 loss: 1.7818693399429322\n",
      " batch 400 loss: 1.778848546743393\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.4:\n",
      "Test accuracy 35.9:\n",
      "EPOCH 9:\n",
      " batch 100 loss: 1.7869526374340057\n",
      " batch 200 loss: 1.7780837166309356\n",
      " batch 300 loss: 1.7937163138389587\n",
      " batch 400 loss: 1.778235855102539\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.3:\n",
      "Test accuracy 36.6:\n",
      "EPOCH 10:\n",
      " batch 100 loss: 1.7727359056472778\n",
      " batch 200 loss: 1.8182347452640533\n",
      " batch 300 loss: 1.789895429611206\n",
      " batch 400 loss: 1.7743658518791199\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.699999999999996:\n",
      "Test accuracy 35.9:\n",
      "EPOCH 11:\n",
      " batch 100 loss: 1.776367690563202\n",
      " batch 200 loss: 1.777634105682373\n",
      " batch 300 loss: 1.7998179769515992\n",
      " batch 400 loss: 1.7792572903633117\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.0:\n",
      "Test accuracy 36.5:\n",
      "EPOCH 12:\n",
      " batch 100 loss: 1.779262466430664\n",
      " batch 200 loss: 1.7750156557559966\n",
      " batch 300 loss: 1.8030779373645782\n",
      " batch 400 loss: 1.775985507965088\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.699999999999996:\n",
      "Test accuracy 36.1:\n",
      "EPOCH 13:\n",
      " batch 100 loss: 1.776769152879715\n",
      " batch 200 loss: 1.772847205400467\n",
      " batch 300 loss: 1.77948016166687\n",
      " batch 400 loss: 1.7681381928920745\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.699999999999996:\n",
      "Test accuracy 36.1:\n",
      "EPOCH 14:\n",
      " batch 100 loss: 1.7655514192581176\n",
      " batch 200 loss: 1.7787785339355469\n",
      " batch 300 loss: 1.794833881855011\n",
      " batch 400 loss: 1.7821366417407989\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.199999999999996:\n",
      "Test accuracy 36.6:\n",
      "EPOCH 15:\n",
      " batch 100 loss: 1.784962944984436\n",
      " batch 200 loss: 1.7673307347297669\n",
      " batch 300 loss: 1.8187277352809905\n",
      " batch 400 loss: 1.7684755074977874\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.1:\n",
      "Test accuracy 36.4:\n",
      "EPOCH 16:\n",
      " batch 100 loss: 1.782850980758667\n",
      " batch 200 loss: 1.7606999719142913\n",
      " batch 300 loss: 1.774859322309494\n",
      " batch 400 loss: 1.7732329761981964\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.0:\n",
      "Test accuracy 35.4:\n",
      "EPOCH 17:\n",
      " batch 100 loss: 1.7978065419197082\n",
      " batch 200 loss: 1.7960691893100738\n",
      " batch 300 loss: 1.7803106749057769\n",
      " batch 400 loss: 1.7546186912059785\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.8:\n",
      "EPOCH 18:\n",
      " batch 100 loss: 1.7798338532447815\n",
      " batch 200 loss: 1.7906371021270753\n",
      " batch 300 loss: 1.780059665441513\n",
      " batch 400 loss: 1.777007088661194\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 34.8:\n",
      "Test accuracy 35.199999999999996:\n",
      "EPOCH 19:\n",
      " batch 100 loss: 1.7816462898254395\n",
      " batch 200 loss: 1.7899803578853608\n",
      " batch 300 loss: 1.7771876871585846\n",
      " batch 400 loss: 1.7573088312149048\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 36.6:\n",
      "Test accuracy 37.1:\n",
      "EPOCH 20:\n",
      " batch 100 loss: 1.7902023136615752\n",
      " batch 200 loss: 1.7774239945411683\n",
      " batch 300 loss: 1.7750326955318452\n",
      " batch 400 loss: 1.7688242256641389\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 34.2:\n",
      "Test accuracy 35.0:\n",
      "EPOCH 21:\n",
      " batch 100 loss: 1.7834695434570313\n",
      " batch 200 loss: 1.7660357499122619\n",
      " batch 300 loss: 1.7896209061145782\n",
      " batch 400 loss: 1.7804449963569642\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 32.9:\n",
      "Test accuracy 33.4:\n",
      "EPOCH 22:\n",
      " batch 100 loss: 1.8088025748729706\n",
      " batch 200 loss: 1.7807401633262634\n",
      " batch 300 loss: 1.797099449634552\n",
      " batch 400 loss: 1.777550389766693\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.6:\n",
      "Test accuracy 36.0:\n",
      "EPOCH 23:\n",
      " batch 100 loss: 1.7803935015201569\n",
      " batch 200 loss: 1.7734255349636079\n",
      " batch 300 loss: 1.7716865587234496\n",
      " batch 400 loss: 1.7636653077602387\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.4:\n",
      "Test accuracy 35.8:\n",
      "EPOCH 24:\n",
      " batch 100 loss: 1.7754412484169007\n",
      " batch 200 loss: 1.7818349385261536\n",
      " batch 300 loss: 1.7785672903060914\n",
      " batch 400 loss: 1.7793074858188629\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 35.199999999999996:\n",
      "Test accuracy 35.4:\n",
      "EPOCH 25:\n",
      " batch 100 loss: 1.7774296975135804\n",
      " batch 200 loss: 1.779866442680359\n",
      " batch 300 loss: 1.7687614715099336\n",
      " batch 400 loss: 1.7741161584854126\n",
      "lr 0.01 -> 0.001\n",
      "Train accuracy 32.4:\n",
      "Test accuracy 33.0:\n",
      "EPOCH 26:\n",
      " batch 100 loss: 1.7676872551441192\n",
      " batch 200 loss: 1.758046258687973\n",
      " batch 300 loss: 1.7370831298828124\n",
      " batch 400 loss: 1.742966158390045\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.199999999999996:\n",
      "Test accuracy 36.4:\n",
      "EPOCH 27:\n",
      " batch 100 loss: 1.7544706606864928\n",
      " batch 200 loss: 1.7240448331832885\n",
      " batch 300 loss: 1.753568024635315\n",
      " batch 400 loss: 1.7473958241939545\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.199999999999996:\n",
      "Test accuracy 36.5:\n",
      "EPOCH 28:\n",
      " batch 100 loss: 1.7418197393417358\n",
      " batch 200 loss: 1.7503027594089509\n",
      " batch 300 loss: 1.7316160714626312\n",
      " batch 400 loss: 1.7475704061985016\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.199999999999996:\n",
      "Test accuracy 36.5:\n",
      "EPOCH 29:\n",
      " batch 100 loss: 1.7476140797138213\n",
      " batch 200 loss: 1.7432787859439849\n",
      " batch 300 loss: 1.7527272284030915\n",
      " batch 400 loss: 1.7335639214515686\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.4:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 30:\n",
      " batch 100 loss: 1.7311607348918914\n",
      " batch 200 loss: 1.7429779088497162\n",
      " batch 300 loss: 1.7530116510391236\n",
      " batch 400 loss: 1.748016780614853\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.199999999999996:\n",
      "Test accuracy 36.4:\n",
      "EPOCH 31:\n",
      " batch 100 loss: 1.7493170487880707\n",
      " batch 200 loss: 1.737138500213623\n",
      " batch 300 loss: 1.7495475828647613\n",
      " batch 400 loss: 1.7336913335323334\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 32:\n",
      " batch 100 loss: 1.7486792850494384\n",
      " batch 200 loss: 1.748419188261032\n",
      " batch 300 loss: 1.7368441092967988\n",
      " batch 400 loss: 1.7327708745002746\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 33:\n",
      " batch 100 loss: 1.7476336467266083\n",
      " batch 200 loss: 1.7423111832141875\n",
      " batch 300 loss: 1.7463522362709045\n",
      " batch 400 loss: 1.7299377703666687\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.3:\n",
      "Test accuracy 36.5:\n",
      "EPOCH 34:\n",
      " batch 100 loss: 1.7224622321128846\n",
      " batch 200 loss: 1.7301273810863496\n",
      " batch 300 loss: 1.7550540137290955\n",
      " batch 400 loss: 1.7507368791103364\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.4:\n",
      "Test accuracy 36.6:\n",
      "EPOCH 35:\n",
      " batch 100 loss: 1.724710178375244\n",
      " batch 200 loss: 1.7362604367733\n",
      " batch 300 loss: 1.7450180506706239\n",
      " batch 400 loss: 1.7421509754657745\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.199999999999996:\n",
      "Test accuracy 36.4:\n",
      "EPOCH 36:\n",
      " batch 100 loss: 1.7508801817893982\n",
      " batch 200 loss: 1.7274255216121674\n",
      " batch 300 loss: 1.7283303332328797\n",
      " batch 400 loss: 1.7597662949562072\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 37:\n",
      " batch 100 loss: 1.7401576447486877\n",
      " batch 200 loss: 1.7325983357429504\n",
      " batch 300 loss: 1.751802659034729\n",
      " batch 400 loss: 1.7341246950626372\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 38:\n",
      " batch 100 loss: 1.7398153913021088\n",
      " batch 200 loss: 1.7379211068153382\n",
      " batch 300 loss: 1.7397297704219818\n",
      " batch 400 loss: 1.729643384218216\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.6:\n",
      "EPOCH 39:\n",
      " batch 100 loss: 1.7431595778465272\n",
      " batch 200 loss: 1.7476843798160553\n",
      " batch 300 loss: 1.723575257062912\n",
      " batch 400 loss: 1.7425195217132567\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.6:\n",
      "Test accuracy 36.8:\n",
      "EPOCH 40:\n",
      " batch 100 loss: 1.729874564409256\n",
      " batch 200 loss: 1.741671483516693\n",
      " batch 300 loss: 1.7418120908737182\n",
      " batch 400 loss: 1.7379149353504182\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 41:\n",
      " batch 100 loss: 1.7446470987796783\n",
      " batch 200 loss: 1.7213523185253143\n",
      " batch 300 loss: 1.741588943004608\n",
      " batch 400 loss: 1.7399993646144867\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.8:\n",
      "EPOCH 42:\n",
      " batch 100 loss: 1.7144721460342407\n",
      " batch 200 loss: 1.7514600086212158\n",
      " batch 300 loss: 1.7360716021060945\n",
      " batch 400 loss: 1.7409889018535614\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.8:\n",
      "EPOCH 43:\n",
      " batch 100 loss: 1.7284040284156799\n",
      " batch 200 loss: 1.742441474199295\n",
      " batch 300 loss: 1.727170876264572\n",
      " batch 400 loss: 1.7487449777126312\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.6:\n",
      "EPOCH 44:\n",
      " batch 100 loss: 1.6290018105506896\n",
      " batch 200 loss: 1.5447642159461976\n",
      " batch 300 loss: 1.5425036251544952\n",
      " batch 400 loss: 1.5306172490119934\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.6:\n",
      "Test accuracy 36.9:\n",
      "EPOCH 45:\n",
      " batch 100 loss: 1.526214327812195\n",
      " batch 200 loss: 1.5325115871429444\n",
      " batch 300 loss: 1.5299823820590972\n",
      " batch 400 loss: 1.5311523830890656\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 46:\n",
      " batch 100 loss: 1.5356991982460022\n",
      " batch 200 loss: 1.5117513597011567\n",
      " batch 300 loss: 1.5172520053386689\n",
      " batch 400 loss: 1.5446252918243408\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 47:\n",
      " batch 100 loss: 1.5275451910495759\n",
      " batch 200 loss: 1.5265500581264495\n",
      " batch 300 loss: 1.5174258744716644\n",
      " batch 400 loss: 1.5359162139892577\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.6:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 48:\n",
      " batch 100 loss: 1.529543422460556\n",
      " batch 200 loss: 1.5272290742397308\n",
      " batch 300 loss: 1.5216902494430542\n",
      " batch 400 loss: 1.5353193187713623\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.6:\n",
      "Test accuracy 36.9:\n",
      "EPOCH 49:\n",
      " batch 100 loss: 1.5173122656345368\n",
      " batch 200 loss: 1.5201679241657258\n",
      " batch 300 loss: 1.532956418991089\n",
      " batch 400 loss: 1.5303601014614105\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 36.5:\n",
      "Test accuracy 36.7:\n",
      "EPOCH 50:\n",
      " batch 100 loss: 1.5304816222190858\n",
      " batch 200 loss: 1.5282265734672547\n",
      " batch 300 loss: 1.5209324610233308\n",
      " batch 400 loss: 1.5201052415370941\n",
      "lr 0.001 -> 0.0001\n",
      "Train accuracy 36.6:\n",
      "Test accuracy 36.7:\n",
      "Optimizer RMSProp:\n",
      "EPOCH 1:\n",
      " batch 100 loss: 3.0158880484104156\n",
      " batch 200 loss: 2.116444407701492\n",
      " batch 300 loss: 2.1038914144039156\n",
      " batch 400 loss: 2.1054248118400576\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 9.9:\n",
      "Test accuracy 9.8:\n",
      "EPOCH 2:\n",
      " batch 100 loss: 2.11285133600235\n",
      " batch 200 loss: 2.1161132037639616\n",
      " batch 300 loss: 2.104947123527527\n",
      " batch 400 loss: 2.1372130250930788\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 9.9:\n",
      "Test accuracy 9.8:\n",
      "EPOCH 3:\n",
      " batch 100 loss: 2.109140535593033\n",
      " batch 200 loss: 2.0988780534267426\n",
      " batch 300 loss: 2.113678183555603\n",
      " batch 400 loss: 2.110841083526611\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 9.9:\n",
      "Test accuracy 9.8:\n",
      "EPOCH 4:\n",
      " batch 100 loss: 2.1032251703739164\n",
      " batch 200 loss: 2.1158472526073457\n",
      " batch 300 loss: 2.1146144330501557\n",
      " batch 400 loss: 2.477447601556778\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 19.0:\n",
      "Test accuracy 18.6:\n",
      "EPOCH 5:\n",
      " batch 100 loss: 1.92392716050148\n",
      " batch 200 loss: 1.9420273804664612\n",
      " batch 300 loss: 1.9461720168590546\n",
      " batch 400 loss: 2.3125660467147826\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 28.4:\n",
      "Test accuracy 28.1:\n",
      "EPOCH 6:\n",
      " batch 100 loss: 1.7233218944072723\n",
      " batch 200 loss: 1.7912534654140473\n",
      " batch 300 loss: 1.7532226896286012\n",
      " batch 400 loss: 1.7418863439559937\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 20.8:\n",
      "Test accuracy 20.5:\n",
      "EPOCH 7:\n",
      " batch 100 loss: 1.7616233491897584\n",
      " batch 200 loss: 1.7849532461166382\n",
      " batch 300 loss: 1.736507281064987\n",
      " batch 400 loss: 1.770634288787842\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 28.4:\n",
      "Test accuracy 27.800000000000004:\n",
      "EPOCH 8:\n",
      " batch 100 loss: 1.745874948501587\n",
      " batch 200 loss: 1.7479276049137116\n",
      " batch 300 loss: 1.7446115231513977\n",
      " batch 400 loss: 1.740014946460724\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 26.900000000000002:\n",
      "Test accuracy 26.5:\n",
      "EPOCH 9:\n",
      " batch 100 loss: 1.7711835169792176\n",
      " batch 200 loss: 1.7393123173713685\n",
      " batch 300 loss: 1.7274527549743652\n",
      " batch 400 loss: 1.7532963466644287\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.3:\n",
      "Test accuracy 26.700000000000003:\n",
      "EPOCH 10:\n",
      " batch 100 loss: 1.7607369685173035\n",
      " batch 200 loss: 1.7534809386730195\n",
      " batch 300 loss: 1.7220142221450805\n",
      " batch 400 loss: 1.7376343750953673\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.900000000000002:\n",
      "Test accuracy 27.3:\n",
      "EPOCH 11:\n",
      " batch 100 loss: 1.736864697933197\n",
      " batch 200 loss: 1.7372193253040313\n",
      " batch 300 loss: 1.7274700367450715\n",
      " batch 400 loss: 1.7518174064159393\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.3:\n",
      "Test accuracy 26.8:\n",
      "EPOCH 12:\n",
      " batch 100 loss: 1.7371640491485596\n",
      " batch 200 loss: 1.7517360186576842\n",
      " batch 300 loss: 1.7247766649723053\n",
      " batch 400 loss: 1.7381141471862793\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.700000000000003:\n",
      "Test accuracy 27.200000000000003:\n",
      "EPOCH 13:\n",
      " batch 100 loss: 1.7242646551132201\n",
      " batch 200 loss: 1.7443420696258545\n",
      " batch 300 loss: 1.7351619291305542\n",
      " batch 400 loss: 1.7291232192516326\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.700000000000003:\n",
      "Test accuracy 27.200000000000003:\n",
      "EPOCH 14:\n",
      " batch 100 loss: 1.7491749107837677\n",
      " batch 200 loss: 1.7230895233154297\n",
      " batch 300 loss: 1.7336342740058899\n",
      " batch 400 loss: 1.7226552891731262\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 25.6:\n",
      "Test accuracy 25.3:\n",
      "EPOCH 15:\n",
      " batch 100 loss: 1.7360225534439087\n",
      " batch 200 loss: 1.7313965356349945\n",
      " batch 300 loss: 1.7326948297023774\n",
      " batch 400 loss: 1.7274213469028472\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.6:\n",
      "EPOCH 16:\n",
      " batch 100 loss: 1.7330952179431915\n",
      " batch 200 loss: 1.7173533153533935\n",
      " batch 300 loss: 1.7510645127296447\n",
      " batch 400 loss: 1.7291819655895233\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.400000000000002:\n",
      "Test accuracy 26.8:\n",
      "EPOCH 17:\n",
      " batch 100 loss: 1.7206520462036132\n",
      " batch 200 loss: 1.7388304162025452\n",
      " batch 300 loss: 1.7252104568481446\n",
      " batch 400 loss: 1.7254578733444215\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.3:\n",
      "Test accuracy 26.8:\n",
      "EPOCH 18:\n",
      " batch 100 loss: 1.740389541387558\n",
      " batch 200 loss: 1.7168759739398955\n",
      " batch 300 loss: 1.7258055305480957\n",
      " batch 400 loss: 1.7168049597740174\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 26.900000000000002:\n",
      "Test accuracy 26.3:\n",
      "EPOCH 19:\n",
      " batch 100 loss: 1.730916486978531\n",
      " batch 200 loss: 1.7319040787220001\n",
      " batch 300 loss: 1.722491034269333\n",
      " batch 400 loss: 1.7195457673072816\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.3:\n",
      "Test accuracy 26.8:\n",
      "EPOCH 20:\n",
      " batch 100 loss: 1.734155457019806\n",
      " batch 200 loss: 1.7272231245040894\n",
      " batch 300 loss: 1.7299909484386444\n",
      " batch 400 loss: 1.7185178756713868\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 28.599999999999998:\n",
      "Test accuracy 28.1:\n",
      "EPOCH 21:\n",
      " batch 100 loss: 1.7198087668418884\n",
      " batch 200 loss: 1.7376667976379394\n",
      " batch 300 loss: 1.7152922546863556\n",
      " batch 400 loss: 1.7239218378067016\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 27.800000000000004:\n",
      "Test accuracy 27.3:\n",
      "EPOCH 22:\n",
      " batch 100 loss: 1.7194653689861297\n",
      " batch 200 loss: 1.7244814586639405\n",
      " batch 300 loss: 1.7162262523174285\n",
      " batch 400 loss: 1.737047084569931\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.6:\n",
      "EPOCH 23:\n",
      " batch 100 loss: 1.722273029088974\n",
      " batch 200 loss: 1.719181854724884\n",
      " batch 300 loss: 1.7264506804943085\n",
      " batch 400 loss: 1.7396724379062654\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.6:\n",
      "EPOCH 24:\n",
      " batch 100 loss: 1.7245461273193359\n",
      " batch 200 loss: 1.7284517145156861\n",
      " batch 300 loss: 1.713254338502884\n",
      " batch 400 loss: 1.732688366174698\n",
      "lr 0.01 -> 0.01\n",
      "Train accuracy 26.400000000000002:\n",
      "Test accuracy 25.8:\n",
      "EPOCH 25:\n",
      " batch 100 loss: 1.7326075375080108\n",
      " batch 200 loss: 1.7200360691547394\n",
      " batch 300 loss: 1.7160005474090576\n",
      " batch 400 loss: 1.7256377410888672\n",
      "lr 0.01 -> 0.001\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.700000000000003:\n",
      "EPOCH 26:\n",
      " batch 100 loss: 1.7053358554840088\n",
      " batch 200 loss: 1.7040705430507659\n",
      " batch 300 loss: 1.7054721212387085\n",
      " batch 400 loss: 1.7095851480960846\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 27.900000000000002:\n",
      "Test accuracy 27.3:\n",
      "EPOCH 27:\n",
      " batch 100 loss: 1.6945853567123412\n",
      " batch 200 loss: 1.6894376361370087\n",
      " batch 300 loss: 1.6845446145534515\n",
      " batch 400 loss: 1.7215114545822143\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 27.900000000000002:\n",
      "Test accuracy 27.400000000000002:\n",
      "EPOCH 28:\n",
      " batch 100 loss: 1.703611309528351\n",
      " batch 200 loss: 1.702102026939392\n",
      " batch 300 loss: 1.6961855971813202\n",
      " batch 400 loss: 1.6855120956897736\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.000000000000004:\n",
      "Test accuracy 27.400000000000002:\n",
      "EPOCH 29:\n",
      " batch 100 loss: 1.6873557472229004\n",
      " batch 200 loss: 1.7022341454029084\n",
      " batch 300 loss: 1.701340935230255\n",
      " batch 400 loss: 1.7021683800220488\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.1:\n",
      "Test accuracy 27.400000000000002:\n",
      "EPOCH 30:\n",
      " batch 100 loss: 1.6972584164142608\n",
      " batch 200 loss: 1.694076269865036\n",
      " batch 300 loss: 1.695678472518921\n",
      " batch 400 loss: 1.7014869093894958\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.1:\n",
      "Test accuracy 27.400000000000002:\n",
      "EPOCH 31:\n",
      " batch 100 loss: 1.675345104932785\n",
      " batch 200 loss: 1.7132369792461395\n",
      " batch 300 loss: 1.7005741274356843\n",
      " batch 400 loss: 1.6947544372081758\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.6:\n",
      "EPOCH 32:\n",
      " batch 100 loss: 1.6945598781108857\n",
      " batch 200 loss: 1.7063770854473115\n",
      " batch 300 loss: 1.6927561962604523\n",
      " batch 400 loss: 1.6806391429901124\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.500000000000004:\n",
      "EPOCH 33:\n",
      " batch 100 loss: 1.6939960062503814\n",
      " batch 200 loss: 1.6864590120315552\n",
      " batch 300 loss: 1.6921669328212738\n",
      " batch 400 loss: 1.6932652115821838\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.4:\n",
      "Test accuracy 27.800000000000004:\n",
      "EPOCH 34:\n",
      " batch 100 loss: 1.6996004116535186\n",
      " batch 200 loss: 1.6949555015563964\n",
      " batch 300 loss: 1.6885685586929322\n",
      " batch 400 loss: 1.6804765665531158\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.1:\n",
      "Test accuracy 27.400000000000002:\n",
      "EPOCH 35:\n",
      " batch 100 loss: 1.6876914978027344\n",
      " batch 200 loss: 1.7038968110084534\n",
      " batch 300 loss: 1.6889171051979064\n",
      " batch 400 loss: 1.7075885331630707\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.299999999999997:\n",
      "Test accuracy 27.700000000000003:\n",
      "EPOCH 36:\n",
      " batch 100 loss: 1.6906998753547668\n",
      " batch 200 loss: 1.7121228110790252\n",
      " batch 300 loss: 1.6787285923957824\n",
      " batch 400 loss: 1.6890139651298524\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.299999999999997:\n",
      "Test accuracy 27.700000000000003:\n",
      "EPOCH 37:\n",
      " batch 100 loss: 1.6963777709007264\n",
      " batch 200 loss: 1.6841744673252106\n",
      " batch 300 loss: 1.6894168663024902\n",
      " batch 400 loss: 1.6841069757938385\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.1:\n",
      "Test accuracy 27.400000000000002:\n",
      "EPOCH 38:\n",
      " batch 100 loss: 1.6851959884166718\n",
      " batch 200 loss: 1.6855523669719696\n",
      " batch 300 loss: 1.6963896024227143\n",
      " batch 400 loss: 1.6935975015163423\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.299999999999997:\n",
      "Test accuracy 27.6:\n",
      "EPOCH 39:\n",
      " batch 100 loss: 1.7010740065574645\n",
      " batch 200 loss: 1.6928255915641786\n",
      " batch 300 loss: 1.6882297444343566\n",
      " batch 400 loss: 1.6875471556186676\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.500000000000004:\n",
      "EPOCH 40:\n",
      " batch 100 loss: 1.6838573980331422\n",
      " batch 200 loss: 1.6890715396404266\n",
      " batch 300 loss: 1.6855656218528747\n",
      " batch 400 loss: 1.6928745353221892\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.199999999999996:\n",
      "Test accuracy 27.500000000000004:\n",
      "EPOCH 41:\n",
      " batch 100 loss: 1.6862977492809295\n",
      " batch 200 loss: 1.6926534724235536\n",
      " batch 300 loss: 1.694450397491455\n",
      " batch 400 loss: 1.6885340416431427\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.4:\n",
      "Test accuracy 27.700000000000003:\n",
      "EPOCH 42:\n",
      " batch 100 loss: 1.6773097538948059\n",
      " batch 200 loss: 1.6975957691669463\n",
      " batch 300 loss: 1.685433212518692\n",
      " batch 400 loss: 1.6851796746253966\n",
      "lr 0.001 -> 0.001\n",
      "Train accuracy 28.4:\n",
      "Test accuracy 27.800000000000004:\n",
      "EPOCH 43:\n",
      " batch 100 loss: 1.7002356386184692\n",
      " batch 200 loss: 1.6906051051616668\n",
      " batch 300 loss: 1.683917692899704\n",
      " batch 400 loss: 1.68719132065773\n"
     ]
    }
   ],
   "source": [
    "learning_rates = 0.01\n",
    "l2reg = 1e-7\n",
    "description, losses_arr, train_accuracy_arr, test_accuracy_arr = train_with_optimizers(learning_rates,l2reg)\n",
    "save_to_csv('Optimizers', description, losses_arr, train_accuracy_arr, test_accuracy_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTcpDV4IV1zs"
   },
   "source": [
    "### Plotting- lr=0.01, bs=128, l2reg=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHCVdQEfp_Ok"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epochs_arr = np.arange(0, len(losses_arr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXEegDFVppvI"
   },
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "x = np.array(epochs_arr)\n",
    "for i,train_loss in enumerate(losses_arr):\n",
    "  ypoints = np.array(train_loss)\n",
    "  plt.plot(x, ypoints, label = name[i])\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"train Loss\")\n",
    "plt.legend()\n",
    "plt.title('LR=0.01, L2Reg=1e-7, Step, Batch size=128')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6es2tlnprwR"
   },
   "outputs": [],
   "source": [
    "# Plot training accuracy\n",
    "x = np.array(epochs_arr)\n",
    "for i,train_acc in enumerate(train_accuracy_arr):\n",
    "  ypoints = np.array(train_acc)\n",
    "  plt.plot(x, ypoints, label = name[i])\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"train Accuracy\")\n",
    "plt.legend()\n",
    "plt.title('LR=0.01, L2Reg=1e-7, Step, Batch size=128')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMzk9B3_ptZu"
   },
   "outputs": [],
   "source": [
    "# Plot test accuracy\n",
    "x = np.array(epochs_arr)\n",
    "for i,test_acc in enumerate(test_accuracy_arr):\n",
    "  ypoints = np.array(test_acc)\n",
    "  plt.plot(x, ypoints, label = name[i])\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "plt.title('LR=0.01, L2Reg=1e-7, Step, Batch size=128')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yOmqZP71qtn"
   },
   "outputs": [],
   "source": [
    "# Plot test and training accuracy\n",
    "x = np.array(epochs_arr)\n",
    "for i,lt in enumerate([0,1]):\n",
    "  ypoints_1 = np.array(train_accuracy_arr[i])\n",
    "  ypoints_2 = np.array(test_accuracy_arr[i])\n",
    "  plt.plot(x, ypoints_1, label = 'Train')\n",
    "  plt.plot(x, ypoints_2, label = 'Test')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.legend()\n",
    "  plt.title(F'LR=0.01, Batch_size=128, L2reg=1e-7, Optimizer={name[i]}')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB38SwpjVkx3"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34bzbZtggYsa"
   },
   "outputs": [],
   "source": [
    "model_save_name = '1695168105559.8071_Step_bs128_e30_SGD_ss10_gamma0.1_mnist'\n",
    "path = F\"{model_save_name}.pt\"\n",
    "net.load_state_dict(t.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT-_48gpVXe9"
   },
   "outputs": [],
   "source": [
    "test(net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJWBQrSQVfay"
   },
   "outputs": [],
   "source": [
    "#hello kuch likhlo warna inactivity timeout de dega yeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PySDxtrTRW_J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMnB1cEKwivNoYwEd1U0ZPr",
   "name": "",
   "provenance": [
    {
     "file_id": "1wGNwoHW3xVOGZBK4XCUdRJB0yqSrAR9-",
     "timestamp": 1695161091996
    },
    {
     "file_id": "15Lmix6u1_Upz2o1ZaphkmkuAmx1gQHy-",
     "timestamp": 1694976575982
    },
    {
     "file_id": "1-6ojp9bMjhfGw9ZcZRZlG8CKheYw_toh",
     "timestamp": 1694894618382
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
